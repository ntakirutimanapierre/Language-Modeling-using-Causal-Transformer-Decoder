{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"deepnote":{},"deepnote_execution_queue":[],"deepnote_notebook_id":"989a3c3836794109ac641230122845a3","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9952835,"sourceType":"datasetVersion","datasetId":6120994}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"widgets":{"application/vnd.jupyter.widget-state+json":{"cf7c55d151b24f91b62405f632224135":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fc6590736b6f4700a977664e21ef9b48","IPY_MODEL_03860fc01f794c929a755c086c92e5fe","IPY_MODEL_4cd51f69265543e39a33960c44dd1c5f"],"layout":"IPY_MODEL_b1f63a0d1e1341748cd590a1780ada8d"}},"fc6590736b6f4700a977664e21ef9b48":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_92f9a691bf7c4245a7c1e1d0f0ffe4c1","placeholder":"​","style":"IPY_MODEL_a9a6ad3a446c4b84b14c8278bd9b04c1","value":"tokenizer_config.json: 100%"}},"03860fc01f794c929a755c086c92e5fe":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_597e7009dd4943c7a94b8283f15da2e8","max":846,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cbae0ca6f84847d9808607106fa30f49","value":846}},"4cd51f69265543e39a33960c44dd1c5f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_69e9e93643484c7ea3697266c5b6f10e","placeholder":"​","style":"IPY_MODEL_66ed0f8e20c04ae586fb5a875bd7db62","value":" 846/846 [00:00&lt;00:00, 59.7kB/s]"}},"b1f63a0d1e1341748cd590a1780ada8d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92f9a691bf7c4245a7c1e1d0f0ffe4c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9a6ad3a446c4b84b14c8278bd9b04c1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"597e7009dd4943c7a94b8283f15da2e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cbae0ca6f84847d9808607106fa30f49":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"69e9e93643484c7ea3697266c5b6f10e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"66ed0f8e20c04ae586fb5a875bd7db62":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"881b6a14efa7484d9494baa5e0aa788d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5db97376ae4e4ce5b43b81266f698f99","IPY_MODEL_ffe3ba1a8ac04885b899190e3add2ad4","IPY_MODEL_06af80bee61743c99a66eaef27092e1e"],"layout":"IPY_MODEL_5683e1228a0042eb8b84a178da5b75ae"}},"5db97376ae4e4ce5b43b81266f698f99":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d3ae8486bb384054b349f3284b735f53","placeholder":"​","style":"IPY_MODEL_cb7719e9b8014c7d919f4cbdbd998bdf","value":"vocab.json: 100%"}},"ffe3ba1a8ac04885b899190e3add2ad4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_79e3e5f690a14bed8cfb23292f755b31","max":146618,"min":0,"orientation":"horizontal","style":"IPY_MODEL_09bd654cce5c48828a1b58ecb15955c0","value":146618}},"06af80bee61743c99a66eaef27092e1e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3aa630a3e003471c9bb4265bb0b3ff8e","placeholder":"​","style":"IPY_MODEL_b55eca50d9694a99b35ba3a5967cbb2b","value":" 147k/147k [00:00&lt;00:00, 324kB/s]"}},"5683e1228a0042eb8b84a178da5b75ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3ae8486bb384054b349f3284b735f53":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb7719e9b8014c7d919f4cbdbd998bdf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"79e3e5f690a14bed8cfb23292f755b31":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09bd654cce5c48828a1b58ecb15955c0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3aa630a3e003471c9bb4265bb0b3ff8e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b55eca50d9694a99b35ba3a5967cbb2b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"316ff234e49b44aaae8982d4bee8ef9b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6120b8558bda4bee895cfe1071476f99","IPY_MODEL_bd7b45e942924b44aece935c11ed8f28","IPY_MODEL_baa1b5e8c741424097695a498973c7a0"],"layout":"IPY_MODEL_af67ac89d5de4e1996c0cfeb2ffb0325"}},"6120b8558bda4bee895cfe1071476f99":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c8a742161f034c53bfc7f04da97d5a3a","placeholder":"​","style":"IPY_MODEL_296a3be8e1f5400b8e0aab780a0adf3b","value":"merges.txt: 100%"}},"bd7b45e942924b44aece935c11ed8f28":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bea5b8a70a6e49e48ae5831f886e0a8a","max":86781,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dc7d47b16f7c4a49bee53bb4538996c2","value":86781}},"baa1b5e8c741424097695a498973c7a0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_39982826915e41c58ceb6f32d9a70846","placeholder":"​","style":"IPY_MODEL_b8fa03486a344dc78141b344404b544a","value":" 86.8k/86.8k [00:00&lt;00:00, 5.14MB/s]"}},"af67ac89d5de4e1996c0cfeb2ffb0325":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8a742161f034c53bfc7f04da97d5a3a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"296a3be8e1f5400b8e0aab780a0adf3b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bea5b8a70a6e49e48ae5831f886e0a8a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc7d47b16f7c4a49bee53bb4538996c2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"39982826915e41c58ceb6f32d9a70846":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8fa03486a344dc78141b344404b544a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1f97e62c9e2c411ea991fba8ddc40461":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a1374db0a75645adad39a9f62ebdb217","IPY_MODEL_210a259cc8f54eb3bd8b21f7a776c4eb","IPY_MODEL_aae7275ea1c2444d83826f2c8ef34c61"],"layout":"IPY_MODEL_f757a408ec67405785b035b2d27a7fc5"}},"a1374db0a75645adad39a9f62ebdb217":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_299f728960af431583fb4caba4c904f7","placeholder":"​","style":"IPY_MODEL_0fdf1d6c392647fe9fee63c8d9a74fdf","value":"tokenizer.json: 100%"}},"210a259cc8f54eb3bd8b21f7a776c4eb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc933872853148cc834af8746f67cb3b","max":684897,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2eb327d922e849f1b56c60179e8b9899","value":684897}},"aae7275ea1c2444d83826f2c8ef34c61":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_009c5aa47fd445e89d0f8e8700fb8d40","placeholder":"​","style":"IPY_MODEL_67e364610cda43b2bd3fe8de8c27b0ca","value":" 685k/685k [00:00&lt;00:00, 822kB/s]"}},"f757a408ec67405785b035b2d27a7fc5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"299f728960af431583fb4caba4c904f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0fdf1d6c392647fe9fee63c8d9a74fdf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cc933872853148cc834af8746f67cb3b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2eb327d922e849f1b56c60179e8b9899":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"009c5aa47fd445e89d0f8e8700fb8d40":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67e364610cda43b2bd3fe8de8c27b0ca":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5b04a79b63b94285b6296adef8daf66f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fee8128f15f44430a8a49141aacea922","IPY_MODEL_d43297a417e849718ecf4a7388afb59c","IPY_MODEL_ffdc0b6dbe5b4b37b13db91ec1f1455e"],"layout":"IPY_MODEL_98be281ecc2745e3ba2deb0aecc0f801"}},"fee8128f15f44430a8a49141aacea922":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e0f31dc72734baabcdde4629d08438e","placeholder":"​","style":"IPY_MODEL_2543f9eaa7fd45c1b035abe638502a74","value":"added_tokens.json: 100%"}},"d43297a417e849718ecf4a7388afb59c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d97f7845edf44361824b7e5ef7d91549","max":27,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0ed723e18a57427fa74fc4e35621e1d6","value":27}},"ffdc0b6dbe5b4b37b13db91ec1f1455e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d9fab8a7677f40628e0370516deac484","placeholder":"​","style":"IPY_MODEL_3aaef6c8f347493aa5d231810109f314","value":" 27.0/27.0 [00:00&lt;00:00, 1.06kB/s]"}},"98be281ecc2745e3ba2deb0aecc0f801":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e0f31dc72734baabcdde4629d08438e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2543f9eaa7fd45c1b035abe638502a74":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d97f7845edf44361824b7e5ef7d91549":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ed723e18a57427fa74fc4e35621e1d6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d9fab8a7677f40628e0370516deac484":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3aaef6c8f347493aa5d231810109f314":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9f897208328e4f74bebf7d26cb249e50":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2107bdc38bc546e3adf7f5719ab76169","IPY_MODEL_65e6c00a56804be88ce3f3e89ac69b10","IPY_MODEL_9edf1accc7c245b89bca2e6519f9fe91"],"layout":"IPY_MODEL_a7e24e8c746a4a7aa8963266c30bb7c2"}},"2107bdc38bc546e3adf7f5719ab76169":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b6a982e45eeb4f9bad975be1f0576978","placeholder":"​","style":"IPY_MODEL_3bfea656aff748299b73b36ebbc34534","value":"special_tokens_map.json: 100%"}},"65e6c00a56804be88ce3f3e89ac69b10":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_383fa8e8ce5a4fd99b41149580a53868","max":621,"min":0,"orientation":"horizontal","style":"IPY_MODEL_42dac938e1834f4e8554b70924c23c61","value":621}},"9edf1accc7c245b89bca2e6519f9fe91":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6740b8cb10d340e6a8a87e8686b5ebb2","placeholder":"​","style":"IPY_MODEL_dc2f4cf040ba49989f6ea16b689f7aa1","value":" 621/621 [00:00&lt;00:00, 24.1kB/s]"}},"a7e24e8c746a4a7aa8963266c30bb7c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b6a982e45eeb4f9bad975be1f0576978":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3bfea656aff748299b73b36ebbc34534":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"383fa8e8ce5a4fd99b41149580a53868":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42dac938e1834f4e8554b70924c23c61":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6740b8cb10d340e6a8a87e8686b5ebb2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc2f4cf040ba49989f6ea16b689f7aa1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"18a3f8ce94ca464f9ee1c67c288dbc08":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c5b4eeb8ebbb440f8ed1e8c27b336404","IPY_MODEL_2abb070d272e4659adadf5cad1ebe1c2","IPY_MODEL_eec5223042304bdab778d8e44c30bf2d"],"layout":"IPY_MODEL_735fbbd3ce934b9eb0880f7590dcf5e0"}},"c5b4eeb8ebbb440f8ed1e8c27b336404":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9082a626184943c89d2ef5d7807890f9","placeholder":"​","style":"IPY_MODEL_64bdc5096d0143de95ecc4bb5f4d6592","value":"100%"}},"2abb070d272e4659adadf5cad1ebe1c2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb2c7f6d1d5948a9951267d620c8caf8","max":56540,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9da5b3eaae894a138154fd8282cf0530","value":56540}},"eec5223042304bdab778d8e44c30bf2d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0168a844bdab4c31a322bc1bdd3e84ed","placeholder":"​","style":"IPY_MODEL_0317d01e93824c7a92bb8ed1acc02ee2","value":" 56540/56540 [22:56&lt;00:00, 36.61it/s]"}},"735fbbd3ce934b9eb0880f7590dcf5e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9082a626184943c89d2ef5d7807890f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64bdc5096d0143de95ecc4bb5f4d6592":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cb2c7f6d1d5948a9951267d620c8caf8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9da5b3eaae894a138154fd8282cf0530":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0168a844bdab4c31a322bc1bdd3e84ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0317d01e93824c7a92bb8ed1acc02ee2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b243a069b4a642dc9dc2ae135c1fa192":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_24ed76dc05e34506bfc22251f486bbda","IPY_MODEL_8ad8ef062adc402a948a80c2cdd9e012","IPY_MODEL_37782ed8b4cc4319b0949ac9e07b5429"],"layout":"IPY_MODEL_b59bee673d3e4d5ab729f7d126fd15c0"}},"24ed76dc05e34506bfc22251f486bbda":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_15475ac4905949f1a54c33c1e583fc8c","placeholder":"​","style":"IPY_MODEL_cb0f8efac0294562a84236a0f9590419","value":"100%"}},"8ad8ef062adc402a948a80c2cdd9e012":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a262c92158884904b0d58d75112f20bd","max":296,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f6cf9e70e18049c0bfb3690f0a0944c3","value":296}},"37782ed8b4cc4319b0949ac9e07b5429":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5214d321021d475fbeb428aa97f28468","placeholder":"​","style":"IPY_MODEL_568c9546a36941a49f2ec96f2896462e","value":" 296/296 [00:02&lt;00:00, 119.98it/s]"}},"b59bee673d3e4d5ab729f7d126fd15c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15475ac4905949f1a54c33c1e583fc8c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb0f8efac0294562a84236a0f9590419":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a262c92158884904b0d58d75112f20bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6cf9e70e18049c0bfb3690f0a0944c3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5214d321021d475fbeb428aa97f28468":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"568c9546a36941a49f2ec96f2896462e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1dbc0248a5774bd9877ae4b8be296294":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4ece5ef93ab94df685d8715fd827c0e3","IPY_MODEL_5a52c1b0c4d44bcaa414a0360a421db2","IPY_MODEL_7659a18c5403419c84a537f99046d1b9"],"layout":"IPY_MODEL_f496fe54db3a4cfdb351591bc8212810"}},"4ece5ef93ab94df685d8715fd827c0e3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_af76e871cbb344d6a9cb5527b32d1361","placeholder":"​","style":"IPY_MODEL_76bc7e7341254bc394a89f428a6ec50d","value":"100%"}},"5a52c1b0c4d44bcaa414a0360a421db2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b3f0a15a19fb4087a94d678261dc2e22","max":56540,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9d90a5ed49cf4b58870ad27ef1b56ca6","value":56540}},"7659a18c5403419c84a537f99046d1b9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2490ba34224f454ea4f81aa5c02544e9","placeholder":"​","style":"IPY_MODEL_50da3d6c024240c3b2e05e0ed6c20c3f","value":" 56540/56540 [23:20&lt;00:00, 32.06it/s]"}},"f496fe54db3a4cfdb351591bc8212810":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af76e871cbb344d6a9cb5527b32d1361":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"76bc7e7341254bc394a89f428a6ec50d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b3f0a15a19fb4087a94d678261dc2e22":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d90a5ed49cf4b58870ad27ef1b56ca6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2490ba34224f454ea4f81aa5c02544e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50da3d6c024240c3b2e05e0ed6c20c3f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"00dbf3604cf040c0a3f9bd2f0bd94c48":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1bc7e9baa9544c61abc0d102ab8703df","IPY_MODEL_11a53878e0ef444ab48fcef8adfa7115","IPY_MODEL_8120a9e572974d6d8d3fef2dfc16331d"],"layout":"IPY_MODEL_f44f50cd6c234b1585f6ed5901aff52a"}},"1bc7e9baa9544c61abc0d102ab8703df":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d1d52ffb6a641bfaf16cbccfa3232c4","placeholder":"​","style":"IPY_MODEL_dc316d211f6540588495f781e42d8d24","value":"100%"}},"11a53878e0ef444ab48fcef8adfa7115":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_652b648e925f4bfeb07a36ee686492bc","max":296,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a6c8fba060ab4ee280225433e6aed599","value":296}},"8120a9e572974d6d8d3fef2dfc16331d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_68849e7132f442e2988d6c364a715ebe","placeholder":"​","style":"IPY_MODEL_23d9b835f6f74397b5f6cf2df1965718","value":" 296/296 [00:01&lt;00:00, 178.06it/s]"}},"f44f50cd6c234b1585f6ed5901aff52a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d1d52ffb6a641bfaf16cbccfa3232c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc316d211f6540588495f781e42d8d24":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"652b648e925f4bfeb07a36ee686492bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6c8fba060ab4ee280225433e6aed599":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"68849e7132f442e2988d6c364a715ebe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23d9b835f6f74397b5f6cf2df1965718":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9e669fd87fad43459d453ebc419b5e4b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6b0ad9dbb7024ccda627ce4cd8a900ee","IPY_MODEL_d2c99448952249dbaee0acc06a05f9af","IPY_MODEL_7a337f4ca15749c1898bd5e9752c18ae"],"layout":"IPY_MODEL_c8da9151489541f5b532069b59402be1"}},"6b0ad9dbb7024ccda627ce4cd8a900ee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b3ce44d13cee444ab7835b663b6cbf91","placeholder":"​","style":"IPY_MODEL_f19d0f3cc8b94d77bd0e0703ac14a7a6","value":"  5%"}},"d2c99448952249dbaee0acc06a05f9af":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a8c22f39d8240aabc33b9adcd26c317","max":56540,"min":0,"orientation":"horizontal","style":"IPY_MODEL_19860cc296944239a427d5c834559d56","value":2826}},"7a337f4ca15749c1898bd5e9752c18ae":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_78fe1b7fcca54930af453938e18e372b","placeholder":"​","style":"IPY_MODEL_02725876537442618e85901d2a297cc2","value":" 2826/56540 [01:12&lt;19:22, 46.21it/s]"}},"c8da9151489541f5b532069b59402be1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b3ce44d13cee444ab7835b663b6cbf91":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f19d0f3cc8b94d77bd0e0703ac14a7a6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4a8c22f39d8240aabc33b9adcd26c317":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19860cc296944239a427d5c834559d56":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"78fe1b7fcca54930af453938e18e372b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02725876537442618e85901d2a297cc2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# HW4P1: Language Modelling\n\n\n\nWelcome to the final part 1 hw of this course. This is the only part 1 in which you have PyTorch training (Yay). You will be working on training language models and evaluating them on the task of prediction and generation.<br>\n\nThe model which you will be coding in this HW very similar to the Speller module from HW4P2.","metadata":{"id":"PSLkT0qL3jgl"}},{"cell_type":"code","source":"# from google.colab import drive\n\n# drive.mount('/content/drive')","metadata":{"executionInfo":{"elapsed":32730,"status":"ok","timestamp":1732103986364,"user":{"displayName":"Pierre Ntakirutimana","userId":"14527179770260799179"},"user_tz":-120},"id":"PoI6RdChOsIC","outputId":"6853a522-4130-4aa4-fb0b-9de6431bed65","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:52:16.325625Z","iopub.execute_input":"2024-11-20T15:52:16.325968Z","iopub.status.idle":"2024-11-20T15:52:16.331279Z","shell.execute_reply.started":"2024-11-20T15:52:16.325936Z","shell.execute_reply":"2024-11-20T15:52:16.329834Z"},"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":"# Get modules and datasets","metadata":{"id":"EB2bOV3bzYLR"}},{"cell_type":"code","source":"!pip install torchsummaryX==1.1.0\n\n!pip install wandb --quiet\n\n!pip install matplotlib\n\n!pip install seaborn\n\n\n\n!pip install transformers -U\n\n!pip install tokenizers","metadata":{"executionInfo":{"elapsed":26578,"status":"ok","timestamp":1732104035096,"user":{"displayName":"Pierre Ntakirutimana","userId":"14527179770260799179"},"user_tz":-120},"id":"r4_-qG9rSULt","outputId":"bf161fb6-90b1-4944-e700-b886cf536fcb","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:52:16.333025Z","iopub.execute_input":"2024-11-20T15:52:16.333871Z","iopub.status.idle":"2024-11-20T15:53:05.190364Z","shell.execute_reply.started":"2024-11-20T15:52:16.333831Z","shell.execute_reply":"2024-11-20T15:53:05.189460Z"},"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: torchsummaryX==1.1.0 in /opt/conda/lib/python3.10/site-packages (1.1.0)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (3.7.5)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: numpy<2,>=1.20 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (10.3.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: seaborn in /opt/conda/lib/python3.10/site-packages (0.12.2)\nRequirement already satisfied: numpy!=1.24.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from seaborn) (1.26.4)\nRequirement already satisfied: pandas>=0.25 in /opt/conda/lib/python3.10/site-packages (from seaborn) (2.2.2)\nRequirement already satisfied: matplotlib!=3.6.1,>=3.1 in /opt/conda/lib/python3.10/site-packages (from seaborn) (3.7.5)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (10.3.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.25->seaborn) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.25->seaborn) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.46.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: tokenizers in /opt/conda/lib/python3.10/site-packages (0.20.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from tokenizers) (0.25.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.8.30)\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"# # TODO: Import drive if you are using Colab\n\n# from google.colab import drive\n\n# drive.mount('/content/drive')","metadata":{"id":"VNTfP7QDYkOP","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:53:05.192139Z","iopub.execute_input":"2024-11-20T15:53:05.192453Z","iopub.status.idle":"2024-11-20T15:53:05.196550Z","shell.execute_reply.started":"2024-11-20T15:53:05.192422Z","shell.execute_reply":"2024-11-20T15:53:05.195715Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"import sys\n\n# path = 'c:\\\\Users\\\\Admin\\\\Desktop\\\\CMU Official Docs\\\\Fall 2\\\\Deep Learning\\\\assignments\\\\HW4\\\\P1\\\\handout\\\\handout'\n\n\n# # TODO: Import drive if you are using Colab\n\n# from google.colab import drive\n\n# drive.mount('/content/drive')\nimport sys\n\npath  = '/kaggle/input/dataset/handout'\n# path = '/content/drive/MyDrive/handout'\n\nsys.path.append(path)\n\n%cd {path}\n\n","metadata":{"executionInfo":{"elapsed":547,"status":"ok","timestamp":1732104097928,"user":{"displayName":"Pierre Ntakirutimana","userId":"14527179770260799179"},"user_tz":-120},"id":"QZNwme4320LW","outputId":"6ff77299-e025-4584-be29-d31bc1942c29","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:53:05.197583Z","iopub.execute_input":"2024-11-20T15:53:05.197880Z","iopub.status.idle":"2024-11-20T15:53:05.210895Z","shell.execute_reply.started":"2024-11-20T15:53:05.197855Z","shell.execute_reply":"2024-11-20T15:53:05.209948Z"},"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"name":"stdout","text":"/kaggle/input/dataset/handout\n","output_type":"stream"}],"execution_count":43},{"cell_type":"markdown","source":"# Imports","metadata":{"id":"INh9p3v3zbF_"}},{"cell_type":"code","source":"%matplotlib inline\n\n\n\nimport torch\n\n\n\nimport os\n\n\n\nimport time\n\nimport math\n\nimport numpy as np\n\nimport pandas as pd\n\nfrom matplotlib import pyplot as plt\n\nimport seaborn as sns\n\nfrom tqdm.notebook import tqdm\n\nimport torchsummaryX\n\nimport torch.nn as nn\n\nimport torch.nn.functional as F\n\nimport math\n\nimport gc\n\nimport glob\n\nimport wandb\n\nimport yaml\n\nimport json\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nfrom transformers import get_linear_schedule_with_warmup\n\n\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nprint(\"Device: \", DEVICE)","metadata":{"executionInfo":{"elapsed":8412,"status":"ok","timestamp":1732104108593,"user":{"displayName":"Pierre Ntakirutimana","userId":"14527179770260799179"},"user_tz":-120},"id":"oxiZ42B4SwQ-","outputId":"46c434b8-56db-47b4-af6c-ccd7b1e5eaa5","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:53:05.212615Z","iopub.execute_input":"2024-11-20T15:53:05.212859Z","iopub.status.idle":"2024-11-20T15:53:05.224800Z","shell.execute_reply.started":"2024-11-20T15:53:05.212835Z","shell.execute_reply":"2024-11-20T15:53:05.223989Z"},"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"name":"stdout","text":"Device:  cuda\n","output_type":"stream"}],"execution_count":44},{"cell_type":"markdown","source":"# Config","metadata":{"id":"loUYgkv8moMl"}},{"cell_type":"code","source":"# config = {\n\n#     \"token_type\":  '10k', # TODO: select a tokenzier from [\"char\", \"1k\", \"10k\"]\n\n#     \"d_model\":     256,\n\n#     \"num_layers\":  2,\n\n#     \"num_heads\":   2,\n\n#     \"d_ff\":        512,\n\n#     \"dropout\":     0.1,\n\n#     \"max_length\":  1000,\n\n#     \"lr\":          0.001,\n\n#     \"batch_size\":  64,\n\n#     \"num_epochs\":  30,\n\n# }\n\n\nconfig = {\n    \"token_type\": \"10k\",  # Use a larger vocabulary for more granular modeling\n    \"d_model\": 768,  # Increase the embedding dimension for richer token representation\n    \"num_layers\": 8,  # Add more layers to improve model capacity\n    \"num_heads\": 12,  # Match the industry standard (e.g., GPT-2) for improved attention granularity\n    \"d_ff\": 3072,  # Increase feed-forward network size for better representation learning\n    \"dropout\": 0.2,  # Increase dropout to reduce overfitting\n    \"max_length\": 512,  # Keep the sequence length consistent\n    \"lr\": 2e-4,  # Reduce learning rate for more stable convergence\n    \"batch_size\": 64,  # Decrease batch size to allow for deeper models with limited GPU memory\n    \"num_epochs\": 50,  # Train longer to allow the model to converge better\n}","metadata":{"id":"ybSqv89zmoMm","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:53:05.225694Z","iopub.execute_input":"2024-11-20T15:53:05.225906Z","iopub.status.idle":"2024-11-20T15:53:05.235368Z","shell.execute_reply.started":"2024-11-20T15:53:05.225884Z","shell.execute_reply":"2024-11-20T15:53:05.234592Z"},"executionInfo":{"status":"ok","timestamp":1732104108593,"user_tz":-120,"elapsed":8,"user":{"displayName":"Pierre Ntakirutimana","userId":"14527179770260799179"}}},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":"","metadata":{"id":"txlgZH4zcASq"}},{"cell_type":"code","source":"","metadata":{"id":"TbK5kbCUP4Tu","trusted":true,"executionInfo":{"status":"ok","timestamp":1732104108593,"user_tz":-120,"elapsed":6,"user":{"displayName":"Pierre Ntakirutimana","userId":"14527179770260799179"}}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load datasets","metadata":{"id":"xdG352aFYdyz"}},{"cell_type":"code","source":"# Define the vocabulary. Try printing and see\n\nVOCAB = [\n\n   \"<sos>\", \"<eos>\",\n\n    \"A\",   \"B\",    \"C\",    \"D\",\n\n    \"E\",   \"F\",    \"G\",    \"H\",\n\n    \"I\",   \"J\",    \"K\",    \"L\",\n\n    \"M\",   \"N\",    \"O\",    \"P\",\n\n    \"Q\",   \"R\",    \"S\",    \"T\",\n\n    \"U\",   \"V\",    \"W\",    \"X\",\n\n    \"Y\",   \"Z\",    \"'\",    \" \", \"<pad>\"\n\n]\n\n\n\nVOCAB_MAP = {VOCAB[i]:i for i in range(0, len(VOCAB))}\n\n# We have also included <sos> and <eos> in the vocabulary for you\n\n# However in real life, you include it explicitly if not provided\n\nPAD_TOKEN =  VOCAB_MAP[\"<pad>\"]\n\nSOS_TOKEN = VOCAB_MAP[\"<sos>\"]\n\nEOS_TOKEN = VOCAB_MAP[\"<eos>\"]\n\n\n\nprint(f\"Length of Vocabulary    : {len(VOCAB)}\")\n\nprint(f\"VOCAB                   : {VOCAB}\")\n\nprint(f\"PAD_TOKEN               : {PAD_TOKEN}\")\n\nprint(f\"SOS_TOKEN               : {SOS_TOKEN}\")\n\nprint(f\"EOS_TOKEN               : {EOS_TOKEN}\")\n\n\n\ndf_train = pd.read_csv(\"dataset/train-clean-100/transcripts.csv\")\n\ndf_val = pd.read_csv(\"dataset/dev-clean/transcripts.csv\")\n\n\n\ndf_train.head()","metadata":{"executionInfo":{"elapsed":6823,"status":"ok","timestamp":1732104115410,"user":{"displayName":"Pierre Ntakirutimana","userId":"14527179770260799179"},"user_tz":-120},"id":"9D39sk7AYdy0","outputId":"c6f0fd13-ff18-400e-adb5-dfb887770291","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:53:05.236416Z","iopub.execute_input":"2024-11-20T15:53:05.236673Z","iopub.status.idle":"2024-11-20T15:53:05.931372Z","shell.execute_reply.started":"2024-11-20T15:53:05.236649Z","shell.execute_reply":"2024-11-20T15:53:05.930511Z"},"colab":{"base_uri":"https://localhost:8080/","height":313}},"outputs":[{"name":"stdout","text":"Length of Vocabulary    : 31\nVOCAB                   : ['<sos>', '<eos>', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', \"'\", ' ', '<pad>']\nPAD_TOKEN               : 30\nSOS_TOKEN               : 0\nEOS_TOKEN               : 1\n","output_type":"stream"},{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0                                        transcripts\n0           0  CHAPTER TEN SHAGGY MAN TO THE RESCUE THEY HAD ...\n1           1  AT ONCE THEY HURRIED FORWARD TO SEE WHAT THIS ...\n2           2  AND PICKED OUT THE EASIEST PLACES TO GO ALL IT...\n3           3  LEAVING HOLES THAT MIGHT CAUSE THE UNWARY TO S...\n4           4  WITH MY HEART RENDING GROWL MY HORRIBLE SHUDDE...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>transcripts</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>CHAPTER TEN SHAGGY MAN TO THE RESCUE THEY HAD ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>AT ONCE THEY HURRIED FORWARD TO SEE WHAT THIS ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>AND PICKED OUT THE EASIEST PLACES TO GO ALL IT...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>LEAVING HOLES THAT MIGHT CAUSE THE UNWARY TO S...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>WITH MY HEART RENDING GROWL MY HORRIBLE SHUDDE...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"class CharTokenizer():\n\n    ''' A wrapper around character tokenization to have a consistent interface with other tokeization strategies'''\n\n\n\n    def __init__(self):\n\n        self.eos_token = \"<|endoftext|>\"  # Same as EOS_TOKEN\n\n        self.pad_token = \"<|padding|>\"\n\n        self.unk_token = \"<|unknown|>\"\n\n\n\n        characters = list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ '\")\n\n\n\n        # Create vocabulary mapping\n\n        self.vocab = {\n\n            self.eos_token: 0,\n\n            self.pad_token: 1,  # Same ID as EOS_TOKEN\n\n            self.unk_token: 2,\n\n        }\n\n\n\n        for idx, char in enumerate(characters, start=3):\n\n            self.vocab[char] = idx\n\n\n\n        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n\n\n\n        self.eos_token_id = self.vocab[self.eos_token]\n\n        self.bos_token_id = self.vocab[self.eos_token]\n\n        self.pad_token_id = self.vocab[self.pad_token]\n\n        self.unk_token_id = self.vocab[self.unk_token]\n\n\n\n        self.vocab_size = len(self.vocab)\n\n\n\n    def tokenize(self, data):\n\n        return [char for char in data]\n\n\n\n    def encode(self, data, return_tensors=None):\n\n        e = [self.vocab.get(char.upper(), self.unk_token) for char in data]\n\n        if return_tensors == 'pt':\n\n            return torch.tensor(e).unsqueeze(0)\n\n        return e\n\n\n\n    def decode(self, data):\n\n        try:\n\n            return ''.join([self.inv_vocab.get(j) for j in data])\n\n        except:\n\n            data = data.cpu().tolist()\n\n            return ''.join([self.inv_vocab.get(j) for j in data])\n\n\n\n    def convert_tokens_to_ids(self, token):\n\n        return self.vocab[token]","metadata":{"id":"9XKR8hhmFslQ","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:53:05.932480Z","iopub.execute_input":"2024-11-20T15:53:05.932751Z","iopub.status.idle":"2024-11-20T15:53:05.941579Z","shell.execute_reply.started":"2024-11-20T15:53:05.932725Z","shell.execute_reply":"2024-11-20T15:53:05.940594Z"},"executionInfo":{"status":"ok","timestamp":1732104115411,"user_tz":-120,"elapsed":13,"user":{"displayName":"Pierre Ntakirutimana","userId":"14527179770260799179"}}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"# load tokenizer\n\nif config[\"token_type\"] == \"1k\":\n\n    TOKENIZER = AutoTokenizer.from_pretrained(\"alexgichamba/hw4_tokenizer_1k\")\n\n    print(\"1k vocab tokenizer loaded\")\n\nelif config[\"token_type\"] == \"10k\":\n\n    TOKENIZER = AutoTokenizer.from_pretrained(\"alexgichamba/hw4_tokenizer_10k\")\n\n    print(\"10k vocab tokenizer loaded\")\n\nelif config[\"token_type\"] == \"char\":\n\n    TOKENIZER = CharTokenizer()\n\n    print(\"character tokenizer loaded\")\n\nelse:\n\n    raise ValueError(\"Invalid token type\")\n\n\n\n\n\nUNK_TOKEN = TOKENIZER.unk_token_id\n\nEOS_TOKEN = TOKENIZER.eos_token_id\n\nSOS_TOKEN = TOKENIZER.bos_token_id\n\nPAD_TOKEN = TOKENIZER.convert_tokens_to_ids('<|padding|>')","metadata":{"executionInfo":{"elapsed":6135,"status":"ok","timestamp":1732104121534,"user":{"displayName":"Pierre Ntakirutimana","userId":"14527179770260799179"},"user_tz":-120},"id":"6kdTvuPfmoMo","outputId":"3f43d9fc-0481-4eec-d84b-bbdef20a2d20","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:53:05.942905Z","iopub.execute_input":"2024-11-20T15:53:05.943146Z","iopub.status.idle":"2024-11-20T15:53:06.540197Z","shell.execute_reply.started":"2024-11-20T15:53:05.943124Z","shell.execute_reply":"2024-11-20T15:53:06.539329Z"},"colab":{"base_uri":"https://localhost:8080/","height":350,"referenced_widgets":["cf7c55d151b24f91b62405f632224135","fc6590736b6f4700a977664e21ef9b48","03860fc01f794c929a755c086c92e5fe","4cd51f69265543e39a33960c44dd1c5f","b1f63a0d1e1341748cd590a1780ada8d","92f9a691bf7c4245a7c1e1d0f0ffe4c1","a9a6ad3a446c4b84b14c8278bd9b04c1","597e7009dd4943c7a94b8283f15da2e8","cbae0ca6f84847d9808607106fa30f49","69e9e93643484c7ea3697266c5b6f10e","66ed0f8e20c04ae586fb5a875bd7db62","881b6a14efa7484d9494baa5e0aa788d","5db97376ae4e4ce5b43b81266f698f99","ffe3ba1a8ac04885b899190e3add2ad4","06af80bee61743c99a66eaef27092e1e","5683e1228a0042eb8b84a178da5b75ae","d3ae8486bb384054b349f3284b735f53","cb7719e9b8014c7d919f4cbdbd998bdf","79e3e5f690a14bed8cfb23292f755b31","09bd654cce5c48828a1b58ecb15955c0","3aa630a3e003471c9bb4265bb0b3ff8e","b55eca50d9694a99b35ba3a5967cbb2b","316ff234e49b44aaae8982d4bee8ef9b","6120b8558bda4bee895cfe1071476f99","bd7b45e942924b44aece935c11ed8f28","baa1b5e8c741424097695a498973c7a0","af67ac89d5de4e1996c0cfeb2ffb0325","c8a742161f034c53bfc7f04da97d5a3a","296a3be8e1f5400b8e0aab780a0adf3b","bea5b8a70a6e49e48ae5831f886e0a8a","dc7d47b16f7c4a49bee53bb4538996c2","39982826915e41c58ceb6f32d9a70846","b8fa03486a344dc78141b344404b544a","1f97e62c9e2c411ea991fba8ddc40461","a1374db0a75645adad39a9f62ebdb217","210a259cc8f54eb3bd8b21f7a776c4eb","aae7275ea1c2444d83826f2c8ef34c61","f757a408ec67405785b035b2d27a7fc5","299f728960af431583fb4caba4c904f7","0fdf1d6c392647fe9fee63c8d9a74fdf","cc933872853148cc834af8746f67cb3b","2eb327d922e849f1b56c60179e8b9899","009c5aa47fd445e89d0f8e8700fb8d40","67e364610cda43b2bd3fe8de8c27b0ca","5b04a79b63b94285b6296adef8daf66f","fee8128f15f44430a8a49141aacea922","d43297a417e849718ecf4a7388afb59c","ffdc0b6dbe5b4b37b13db91ec1f1455e","98be281ecc2745e3ba2deb0aecc0f801","5e0f31dc72734baabcdde4629d08438e","2543f9eaa7fd45c1b035abe638502a74","d97f7845edf44361824b7e5ef7d91549","0ed723e18a57427fa74fc4e35621e1d6","d9fab8a7677f40628e0370516deac484","3aaef6c8f347493aa5d231810109f314","9f897208328e4f74bebf7d26cb249e50","2107bdc38bc546e3adf7f5719ab76169","65e6c00a56804be88ce3f3e89ac69b10","9edf1accc7c245b89bca2e6519f9fe91","a7e24e8c746a4a7aa8963266c30bb7c2","b6a982e45eeb4f9bad975be1f0576978","3bfea656aff748299b73b36ebbc34534","383fa8e8ce5a4fd99b41149580a53868","42dac938e1834f4e8554b70924c23c61","6740b8cb10d340e6a8a87e8686b5ebb2","dc2f4cf040ba49989f6ea16b689f7aa1"]}},"outputs":[{"name":"stdout","text":"10k vocab tokenizer loaded\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"train_transcripts = [np.array([i for i in row['transcripts'].replace(\"<sos>\", \"\").replace(\"<eos>\", \"\") ]) for index, row in df_train.iterrows()]\n\ntrain_dataset = []\n\nfor files in train_transcripts:\n\n    tokenized = \"\".join(files)\n\n    tokenized = TOKENIZER.encode(tokenized)\n\n    train_dataset.append(tokenized)\n\n\n\nval_transcripts = [np.array([i for i in row['transcripts'].replace(\"<sos>\", \"\").replace(\"<eos>\", \"\") ]) for index, row in df_val.iterrows()]\n\nval_dataset = []\n\nfor files in val_transcripts:\n\n    tokenized = \"\".join(files)\n\n    tokenized = TOKENIZER.encode(tokenized)\n\n    val_dataset.append(tokenized)\n\n\n\nprint(len(train_dataset))\n\nprint(len(val_dataset))","metadata":{"executionInfo":{"elapsed":83420,"status":"ok","timestamp":1732104204949,"user":{"displayName":"Pierre Ntakirutimana","userId":"14527179770260799179"},"user_tz":-120},"id":"csVEvmoc3pAI","outputId":"5bcaf45e-a1c8-40b8-e374-e7183029e34d","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:53:06.541235Z","iopub.execute_input":"2024-11-20T15:53:06.541526Z","iopub.status.idle":"2024-11-20T15:54:14.878790Z","shell.execute_reply.started":"2024-11-20T15:53:06.541499Z","shell.execute_reply":"2024-11-20T15:54:14.877844Z"},"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"name":"stdout","text":"281241\n2433\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"# a = 0\n\n# while a < 10:\n\n#     print(val_dataset[a])\n\n#     a += 1\n\n\n\na = [word for article in val_dataset for word in article]\n\nlen(a)//(3*64)","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1732104204949,"user":{"displayName":"Pierre Ntakirutimana","userId":"14527179770260799179"},"user_tz":-120},"id":"XDptw7QxOEvI","outputId":"06b140ff-e1a1-4a05-aa92-fe2f9bd58aaf","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:54:14.882126Z","iopub.execute_input":"2024-11-20T15:54:14.882466Z","iopub.status.idle":"2024-11-20T15:54:14.891592Z","shell.execute_reply.started":"2024-11-20T15:54:14.882435Z","shell.execute_reply":"2024-11-20T15:54:14.890607Z"},"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"296"},"metadata":{}}],"execution_count":50},{"cell_type":"code","source":"VOCAB_SIZE = TOKENIZER.vocab_size\n\n\n\n# test the tokenizer\n\nif TOKENIZER is not None:\n\n    TOKENIZER.decode([EOS_TOKEN, SOS_TOKEN, PAD_TOKEN, UNK_TOKEN])\n\n    print(TOKENIZER.tokenize(\"HELLO DEEP LEARNERS\"))","metadata":{"executionInfo":{"elapsed":3539,"status":"ok","timestamp":1732104208485,"user":{"displayName":"Pierre Ntakirutimana","userId":"14527179770260799179"},"user_tz":-120},"id":"umQYdUCjmoMo","outputId":"cfc5bb6f-a49a-461a-9a67-b3ab73a8524f","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:54:14.892736Z","iopub.execute_input":"2024-11-20T15:54:14.893031Z","iopub.status.idle":"2024-11-20T15:54:14.902709Z","shell.execute_reply.started":"2024-11-20T15:54:14.893003Z","shell.execute_reply":"2024-11-20T15:54:14.901930Z"},"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"name":"stdout","text":"['HE', 'LLO', 'ĠDEEP', 'ĠLEARN', 'ERS']\n","output_type":"stream"}],"execution_count":51},{"cell_type":"markdown","source":"# Custom DataLoader","metadata":{"id":"dHjYhXAOzkrP"}},{"cell_type":"code","source":"# class DataLoaderForLanguageModeling(torch.utils.data.DataLoader): # Inherit from torch.utils.data.DataLoader\n\n#     \"\"\"\n\n#         TODO: Define data loader logic here\n\n#     \"\"\"\n\n#     # TODO: You can probably add more parameters as well. Eg. sequence length\n\n#     def __init__(self, dataset, batch_size, sequence_length=3, shuffle= True, drop_last= False):\n\n\n\n#         # If you remember, these are the standard things which you give while defining a dataloader.\n\n#         # Now you are just customizing your dataloader\n\n#         self.dataset    = dataset\n\n#         self.batch_size = batch_size\n\n#         self.shuffle    = shuffle\n\n#         self.drop_last  = drop_last\n\n#         self.sequence_length = sequence_length\n\n\n\n#         # Flatten the dataset\n\n#         self.flat_data = [word for article in self.dataset for word in article]\n\n\n\n#         # Calculate the number of sequences\n\n#         self.num_sequences = len(self.flat_data) // self.sequence_length\n\n\n\n#         # Calculate the number of batches\n\n#         self.num_batches = self.num_sequences // self.batch_size\n\n\n\n#         if not self.drop_last and self.num_sequences % self.batch_size != 0:\n\n#             self.num_batches += 1\n\n\n\n\n\n#     def __len__(self):\n\n#         # What output do you get when you print len(loader)? You get the number of batches\n\n#         # Your dataset has (579, ) articles and each article has a specified amount of words.\n\n#         # You concatenate the dataset and then batch parts of it according to the sequence length\n\n#         # TODO: return the number of batches\n\n#         # If you are using variable sequence_length, the length might not be fixed\n\n#         return self.num_batches\n\n\n\n\n\n#     def __iter__(self):\n\n#         # TODO: Shuffle data if shuffle is True\n\n#         self.dataset = np.array(self.flat_data)  #np.concatenate(self.dataset, axis=0)\n\n#         if self.shuffle:\n\n#             np.random.shuffle(self.dataset)\n\n\n\n#         # TODO: Concatenate articles drop extra words that won't fit into a full batch\n\n\n\n\n\n#         if self.drop_last:\n\n#             self.dataset = self.dataset[:self.num_batches*self.batch_size*self.sequence_length]\n\n#         else:\n\n#             pass\n\n\n\n#         # TODO: Divide the concetenated dataset into inputs and targets. How do they vary?\n\n#         inputs = []\n\n#         targets = []\n\n\n\n#         for batch in range(self.num_batches):\n\n#             inputs = self.dataset[batch*self.batch_size * self.sequence_length:(batch+1)*self.batch_size * self.sequence_length]\n\n#             targets = self.dataset[batch*self.batch_size * self.sequence_length + 1:(batch+1)*self.batch_size * self.sequence_length +1]\n\n#             inputs = torch.tensor(inputs, dtype=torch.long)\n\n#             targets = torch.tensor(targets, dtype=torch.long)\n\n#             inputs = inputs.reshape(((self.batch_size, self.sequence_length)))\n\n#             targets = targets.reshape(((self.batch_size, self.sequence_length)))\n\n\n\n#             yield inputs, targets\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","metadata":{"id":"gH_fPT-fYdy1","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:54:14.904084Z","iopub.execute_input":"2024-11-20T15:54:14.904367Z","iopub.status.idle":"2024-11-20T15:54:14.912511Z","shell.execute_reply.started":"2024-11-20T15:54:14.904341Z","shell.execute_reply":"2024-11-20T15:54:14.911706Z"},"executionInfo":{"status":"ok","timestamp":1732104208485,"user_tz":-120,"elapsed":3,"user":{"displayName":"Pierre Ntakirutimana","userId":"14527179770260799179"}}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"from torch.utils.data.dataloader import Dataset\n\nclass DataLoaderForLanguageModeling(torch.utils.data.DataLoader): # Inherit from torch.utils.data.DataLoader\n\n    \"\"\"\n\n        TODO: Define data loader logic here\n\n    \"\"\"\n\n    # TODO: You can probably add more parameters as well. Eg. sequence length\n\n    def __init__(self, dataset, batch_size,sequence_length, shuffle= True, drop_last= False):\n\n\n\n        # If you remember, these are the standard things which you give while defining a dataloader.\n\n        # Now you are just customizing your dataloader\n\n        self.dataset    = dataset\n\n        self.batch_size = batch_size\n\n        self.shuffle    = shuffle\n\n        self.drop_last  = drop_last\n\n        self.sequence_length = sequence_length\n\n\n\n        self.dataset = np.concatenate(dataset, axis=0)\n\n        self.num_batches = len(self.dataset) // (self.batch_size * self.sequence_length)\n\n\n\n    def __len__(self):\n\n        # What output do you get when you print len(loader)? You get the number of batches\n\n        # Your dataset has (579, ) articles and each article has a specified amount of words.\n\n        # You concatenate the dataset and then batch parts of it according to the sequence length\n\n        # TODO: return the number of batches\n\n        # If you are using variable sequence_length, the length might not be fixed\n\n        return self.num_batches\n\n\n\n    def __iter__(self):\n\n        # TODOs:\n\n        # 1. Shuffle data if shuffle is True\n\n        # 2. Concatenate articles and drop extra words\n\n        # 3. Divide the concetenated dataset into inputs and targets. How do they vary?\n\n        # 4. Reshape the inputs and targets into batches (think about the final shape)\n\n        # 5. Loop though the batches and yield the input and target according to the sequence length\n\n\n\n        if self.shuffle:\n\n            # TODO\n\n            np.random.shuffle(self.dataset)\n\n\n\n        batch_idx = 0\n\n        inputs = []\n\n        targets = []\n\n        input_length = self.batch_size * self.sequence_length\n\n\n\n        while batch_idx < self.num_batches:\n\n          inputs = self.dataset[batch_idx*input_length:(batch_idx+1)*input_length]\n\n          targets = self.dataset[batch_idx*input_length+1:(batch_idx+1)*input_length+1]\n\n          inputs = torch.tensor(inputs, dtype=torch.long)\n\n          targets = torch.tensor(targets, dtype=torch.long)\n\n          inputs = inputs.reshape(((self.batch_size, self.sequence_length)))\n\n          targets = targets.reshape(((self.batch_size, self.sequence_length)))\n\n          yield inputs, targets\n\n          batch_idx+=1","metadata":{"id":"A4vXqnFLY5iQ","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:54:14.913730Z","iopub.execute_input":"2024-11-20T15:54:14.914002Z","iopub.status.idle":"2024-11-20T15:54:14.923798Z","shell.execute_reply.started":"2024-11-20T15:54:14.913977Z","shell.execute_reply":"2024-11-20T15:54:14.923046Z"},"executionInfo":{"status":"ok","timestamp":1732104225599,"user_tz":-120,"elapsed":780,"user":{"displayName":"Pierre Ntakirutimana","userId":"14527179770260799179"}}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"import random\n\nclass DataLoaderForLanguageModeling(torch.utils.data.DataLoader): # Inherit from torch.utils.data.DataLoader\n    \"\"\"\n        TODO: Define data loader logic here\n    \"\"\"\n    # TODO: You can probably add more parameters as well. Eg. sequence length\n    def __init__(self, dataset, batch_size, sequence_length=3, shuffle= True, drop_last= False):\n\n        # If you remember, these are the standard things which you give while defining a dataloader.\n        # Now you are just customizing your dataloader\n        self.dataset    = dataset\n        self.batch_size = batch_size\n        self.shuffle    = shuffle\n        self.drop_last  = drop_last\n        self.sequence_length = sequence_length\n\n\n    def __len__(self):\n        # What output do you get when you print len(loader)? You get the number of batches\n        # Your dataset has (579, ) articles and each article has a specified amount of words.\n        # You concatenate the dataset and then batch parts of it according to the sequence length\n        # TODO: return the number of batches\n        # If you are using variable sequence_length, the length might not be fixed\n        tokens = sum([len(seq) for seq in self.dataset])\n        n = (tokens - 1) // self.sequence_length\n        return n // self.batch_size + (0 if self.drop_last else 1)\n\n    def __iter__(self):\n        # TODO: Shuffle data if shuffle is True\n        if self.shuffle:\n            random.shuffle(self.dataset)\n\n        # TODO: Concatenate articles drop extra words that won't fit into a full batch\n        self.concatenated_dataset = []\n        for seq in self.dataset:\n            self.concatenated_dataset.extend(seq)\n\n        self.num_batches = self.__len__()\n\n        total_sequences = (len(self.concatenated_dataset) - 1) // self.sequence_length\n        usable = total_sequences * self.sequence_length\n        if self.drop_last:\n            usable = (usable // self.batch_size) * self.batch_size\n            self.concatenated_dataset = self.concatenated_dataset[:usable + 1]\n        # else:\n        #     # Pad the last target sequence with EOS_TOKEN to ensure it has the same length as the other target sequences\n        #     pass\n\n        # TODO: Divide the concetenated dataset into inputs and targets. How do they vary?\n        n_rows = self.batch_size\n        n_cols = len(self.concatenated_dataset) // n_rows\n        self.concatenated_dataset = self.concatenated_dataset[:n_rows*n_cols]\n        data = torch.tensor(self.concatenated_dataset, dtype=torch.long).view(n_rows, -1)\n        # print(\"here\")\n\n        # TODO: Reshape the inputs and targets into batches (think about the final shape)\n\n        # TODO: Loop though the batches and yield the input and target according to the sequence length\n        # pass\n        for i in range(0, data.size(1) - self.sequence_length, self.sequence_length):\n            input_sequence = data[:, i:i+self.sequence_length]\n            target_sequence = data[:, i+1:i+self.sequence_length+1]\n            yield input_sequence, target_sequence\n\n    def get_batch_info(self):\n        return {\n            \"batch size\": self.batch_size,\n            \"sequence length\": self.sequence_length,\n            \"shuffle\": self.shuffle,\n            \"drop last\": self.drop_last,\n            \"length\": self.__len__()\n        }","metadata":{"id":"AW1HpiAPfCnJ","executionInfo":{"status":"ok","timestamp":1732104793147,"user_tz":-120,"elapsed":660,"user":{"displayName":"Pierre Ntakirutimana","userId":"14527179770260799179"}},"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:54:14.924785Z","iopub.execute_input":"2024-11-20T15:54:14.925123Z","iopub.status.idle":"2024-11-20T15:54:14.937884Z","shell.execute_reply.started":"2024-11-20T15:54:14.925084Z","shell.execute_reply":"2024-11-20T15:54:14.937113Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"dl = DataLoaderForLanguageModeling(\n\n    dataset = train_dataset,\n\n    batch_size = config['batch_size'],\n\n    shuffle = True,\n\n    drop_last = False,\n\n    sequence_length = 3\n\n    # Input Extra parameters here if needed\n\n)","metadata":{"id":"t7IyJOYqYdy1","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:54:14.938867Z","iopub.execute_input":"2024-11-20T15:54:14.939111Z","iopub.status.idle":"2024-11-20T15:54:15.153057Z","shell.execute_reply.started":"2024-11-20T15:54:14.939087Z","shell.execute_reply":"2024-11-20T15:54:15.151986Z"},"executionInfo":{"status":"ok","timestamp":1732104793729,"user_tz":-120,"elapsed":3,"user":{"displayName":"Pierre Ntakirutimana","userId":"14527179770260799179"}}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"# inputs, targets = next(iter(dl))\n\n# print(inputs.shape, targets.shape)\n\n\n\n# for x, y in dl:\n\n#     print(\"x: \", [VOCAB[i] for i in x[0, :]])\n\n#     print(\"y: \", [VOCAB[i] for i in y[0, :]])\n\n#     break","metadata":{"executionInfo":{"elapsed":774,"status":"ok","timestamp":1732104796705,"user":{"displayName":"Pierre Ntakirutimana","userId":"14527179770260799179"},"user_tz":-120},"id":"3Zed6Jt7OEvJ","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:54:15.154164Z","iopub.execute_input":"2024-11-20T15:54:15.154468Z","iopub.status.idle":"2024-11-20T15:54:15.162685Z","shell.execute_reply.started":"2024-11-20T15:54:15.154430Z","shell.execute_reply":"2024-11-20T15:54:15.161759Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"# Some sanity checks\n\n\n\ninputs, targets = next(iter(dl))\n\nprint(inputs.shape, targets.shape)\n\nfor x, y in dl:\n\n    transcript = TOKENIZER.decode(x[0].tolist())\n\n    transcript_y = TOKENIZER.decode(y[0].tolist())\n\n    print(\"x: \", transcript)\n\n    print(\"y: \", transcript_y)\n\n    break","metadata":{"executionInfo":{"elapsed":5348,"status":"ok","timestamp":1732104803441,"user":{"displayName":"Pierre Ntakirutimana","userId":"14527179770260799179"},"user_tz":-120},"id":"fBZSzmy10M9M","outputId":"b28656cf-1948-4f05-cc50-1f808c82821c","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:54:15.163719Z","iopub.execute_input":"2024-11-20T15:54:15.164002Z","iopub.status.idle":"2024-11-20T15:54:18.518010Z","shell.execute_reply.started":"2024-11-20T15:54:15.163977Z","shell.execute_reply":"2024-11-20T15:54:18.517129Z"},"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"name":"stdout","text":"torch.Size([64, 3]) torch.Size([64, 3])\nx:  WHAT WAS PERFECT\ny:   WAS PERFECT EMPIRE\n","output_type":"stream"}],"execution_count":57},{"cell_type":"markdown","source":"# Causal Language Model","metadata":{"id":"WcWU0YlnzmVM"}},{"cell_type":"markdown","source":"Causal language models predict the probability of a word based on the preceding words in the sentence. This differs from bidirectional models, which consider both previous and following context. Here, we use a Transformer-based decoder, leveraging its attention mechanism to focus only on earlier parts of the sequence to predict the next word. This type of modeling is suitable for tasks such as text generation where the sequence order is crucial.\n\n\n\n\n\n**Link to HuggingFace Documentation**: [Causal Language Model](https://huggingface.co/docs/transformers/en/tasks/language_modeling)\n\n\n\nThe following image can be a helpful aid in visualizing the flow of information in a causal language model, highlighting how each word in a sequence is used to predict the next word.\n\n\n\n<img src=\"https://github.com/christianversloot/machine-learning-articles/blob/main/images/causal-1024x445.png?raw=true\" width=\"60%\">\n\n\n\nThis figure shows three matrices: the attention scores between sequence elements, the causal mask with zeros allowing attention and negative infinity blocking future attention, and the resultant matrix after applying the causal mask. The negative infinity values in the causal mask prevent the model from using future tokens in its predictions, reinforcing the sequence's order. This visualization shows how transformers can be used for causal language modeling where future input information must not influence current predictions.\n\n\n\n<img src=\"https://github.com/christianversloot/machine-learning-articles/raw/main/images/Diagram-20-1024x282.png\" width=\"80%\">","metadata":{"id":"_7wwkDXlV3xf"}},{"cell_type":"markdown","source":"## 2. `create_mask`: Mask for Preventing Attention to Subsequent Positions\n\n\n\n\n\n```python\n\ndef create_mask(seq, pad_idx=None)\n\n```\n\n\n\n\n\n## Purpose:\n\nThis function creates a **subsequent mask** that prevents attention from attending to future positions in the sequence. It ensures that each position can only attend to previous positions (as in causal language modeling).\n\n\n\n## Usage:\n\n- **Input:**\n\n  - `seq`: Tensor of shape `(batch_size, sequence_length)` representing the input sequence.\n\n  - `pad_idx`: (Optional) Padding index for masking padding positions.\n\n\n\n- **Output:**\n\n  - A mask of shape `(batch_size, sequence_length, sequence_length)` where the upper triangular portion is filled with 1s to prevent attention to future positions.\n\n\n\n### The Expected mask should look like the image below:\n\n\n\n<img src=\"https://i.imgur.com/AJdqMGx.png\" alt=\"drawing\" width=\"400\"/>","metadata":{"id":"dps6lGqiYdy1"}},{"cell_type":"code","source":"def create_mask(seq, pad_idx=None):\n\n    \"\"\" Create a mask to prevent positions from attending to subsequent positions.\n\n\n\n    Args:\n\n        seq: The input sequence tensor, shape (batch_size, sequence_length).\n\n\n\n    Returns:\n\n        A mask tensor with shape (batch_size, sequence_length, sequence_length),\n\n            where positions are allowed to attend to previous positions but not to subsequent positions.\n\n    \"\"\"\n\n\n\n    sz_b, len_s = seq.size()\n\n\n\n    # Create an upper triangular matrix with zeros on the diagonal and below (indicating allowed positions)\n\n    #   and ones above the diagonal (indicating disallowed positions)\n\n\n\n    subsequent_mask =  torch.triu(torch.ones(len_s, len_s, device=seq.device), diagonal=1) # TODO\n\n\n\n    # Expand the mask to match the batch size, resulting in a mask for each sequence in the batch.\n\n    mask = subsequent_mask.unsqueeze(0).expand(sz_b, -1, -1)  # b x ls x ls\n\n\n\n\n\n    ''' Create a mask to ignore padding positions in the key sequence during attention calculation. '''\n\n\n\n    # Expanding to fit the shape of key query attention matrix.\n\n    if pad_idx != None:\n\n        len_q = seq.size(1)\n\n\n\n          # Create a mask where padding positions in the key sequence are marked with 1.\n\n        padding_mask  = seq.eq(pad_idx)\n\n\n\n          # Expand the mask to match the dimensions of the key-query attention matrix.\n\n        padding_mask  = padding_mask.unsqueeze(1).expand(-1, len_q, -1)  # b x lq x lk\n\n\n\n\n\n        mask          = (padding_mask + mask).gt(0)\n\n\n\n    else:\n\n        mask = mask.gt(0)\n\n\n\n    return mask\n\n\n\n\n\n\n\n\n\n\n\n\n\ndec_causal_mask         = create_mask( torch.randn(4, 10)  , pad_idx=0)\n\n\n\n# Black portions are attended to\n\nfig, axs = plt.subplots(1, 1, figsize=(5, 5))\n\n\n\naxs.imshow(dec_causal_mask[0], cmap=\"gray\", aspect='auto')\n\naxs.set_title(\"Decoder Causal Self-Attn Mask\")\n\n\n\n\n\nplt.show()\n\n","metadata":{"executionInfo":{"elapsed":728,"status":"ok","timestamp":1732104810115,"user":{"displayName":"Pierre Ntakirutimana","userId":"14527179770260799179"},"user_tz":-120},"id":"v49yRYF7Ydy1","outputId":"36d9d28f-4bfe-4c83-965d-b0ccc37669b4","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:54:18.519256Z","iopub.execute_input":"2024-11-20T15:54:18.519892Z","iopub.status.idle":"2024-11-20T15:54:19.058309Z","shell.execute_reply.started":"2024-11-20T15:54:18.519845Z","shell.execute_reply":"2024-11-20T15:54:19.057554Z"},"colab":{"base_uri":"https://localhost:8080/","height":468}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 500x500 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAaoAAAHDCAYAAAB1dF5kAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAnWUlEQVR4nO3deXTU9b3/8dckkEkuSUa2gJhAAlgjEAEJUImspuRgoEUKWjYJ9GKvhv1ihXoAFSGmXhEuKCDXIoUgVgSkttRSNhXxsiOLyCYYEAJhyQTUCMnn9we/zHXMQiYs8wl5Ps6Z48l3vjPzzpdJnn7n+82MwxhjBACApQL8PQAAAKUhVAAAqxEqAIDVCBUAwGqECgBgNUIFALAaoQIAWI1QAQCsRqgAAFYjVLguR48elcPh0FtvveXvUSoUf2635557Tg6Hw2vZlStX9Pvf/15RUVEKCAhQz549b/lct4u33npLDodDW7du9fcotw1C5WeFT+rCS3BwsOrVq6ekpCT993//t3Jzc/09onXcbreef/55NW/eXKGhoQoJCVGzZs30zDPP6JtvvvH3eDfc0aNHNXjwYDVq1EjBwcGqW7euOnTooEmTJt2wx/jTn/6kl19+Wb1799aCBQs0evToa97m9ddfl8PhUNu2bYu9ft++fXruued09OjRYm97MyJdGOGAgABlZmYWud7tdiskJEQOh0PDhg274Y+Pm6OKvwfAVS+88IJiYmJ0+fJlnTp1SuvXr9eoUaM0bdo0rVy5Uvfdd5+/R7TCkSNHlJiYqK+//lp9+vTRE088oaCgIH3++ed68803tXz5ch04cMDfY94whw4dUuvWrRUSEqIhQ4YoOjpaJ0+e1Pbt25Wenq7nn3/+hjzO2rVrddddd+nVV18t820yMjIUHR2tzZs369ChQ2rcuLHX9fv27dPzzz+vTp06KTo62uu6119/XbVq1VJKSsoNmL4op9Opt99+W7///e+9li9btuymPB5uLkJliW7duik+Pt7z9fjx47V27Vp1795dv/zlL/XFF18oJCTEjxPeGpcuXVK1atWKve7KlSvq1auXsrKytH79ej344INe10+ZMkXp6em3Ysxb5tVXX9XFixe1c+dONWjQwOu606dP37DHOX36tO64444yr//VV1/p008/1bJly/S73/1OGRkZN3QP73o9/PDDxYZq8eLFSk5O1nvvveenyVAevPRnsS5dumjChAk6duyYFi1a5HXd/v371bt3b9WoUUPBwcGKj4/XypUri9zHhQsXNHr0aEVHR8vpdCoyMlKPP/64srOzPeucPn1av/3tb1WnTh0FBwerefPmWrBgQbH3lZKSIpfLpTvuuEODBg3ShQsXip29LPMVvuy5YcMGPfXUU4qIiFBkZGSJ2+O9997Trl279OyzzxaJlCSFh4drypQpnq8//vhj9enTR/Xr15fT6VRUVJRGjx6t7777zut2nTp1UqdOnYrcX0pKSpE9gSVLlqhVq1YKCwtTeHi44uLiNGPGDM/1586d09ixYxUXF6fQ0FCFh4erW7du2rVrV4nfV2kOHz6syMjIIpGSpIiIiCLLVq1apfbt26tatWoKCwtTcnKy9u7dW+L9Fx4rW7dunfbu3et5CXr9+vWlzpWRkaHq1asrOTlZvXv3VkZGhtf1b731lvr06SNJ6ty5s9f9RkdHa+/evdqwYYNneeH2L3xObNy4UWPGjFHt2rVVrVo1PfLIIzpz5sw1ttb/6devn3bu3Kn9+/d7lp06dUpr165Vv379iqz/ww8/aOLEiWrVqpVcLpeqVaum9u3ba926dUXWvdZzoDjnz59XmzZtFBkZqS+//LLM3weuIlSWGzhwoCTpn//8p2fZ3r179fOf/1xffPGFxo0bp1deeUXVqlVTz549tXz5cs96Fy9eVPv27TVz5kx17dpVM2bM0H/8x39o//79On78uCTpu+++U6dOnbRw4UL1799fL7/8slwul1JSUrx++Iwx+tWvfqWFCxdqwIABevHFF3X8+HENGjSoyMxlna/QU089pX379mnixIkaN25ciduiMHSF2+Ra3n33XX377bd68sknNXPmTCUlJWnmzJl6/PHHy3T7n1q9erX69u2r6tWrKz09XS+99JI6deqkjRs3etY5cuSIVqxYoe7du2vatGl6+umntXv3bnXs2LFcx88aNGigzMxMrV279prrLly4UMnJyQoNDVV6eromTJigffv26cEHHyz2OJEk1a5dWwsXLlRsbKwiIyO1cOFCLVy4UPfee2+pj5WRkaFevXopKChIffv21cGDB7VlyxbP9R06dNCIESMkSX/4wx+87nf69OmKjIxUbGysZ/mzzz7rdf/Dhw/Xrl27NGnSJD355JP661//6tMxpQ4dOigyMlKLFy/2LHvnnXcUGhqq5OTkIuu73W79z//8jzp16qT09HQ999xzOnPmjJKSkrRz507PemV5DvxUdna2unTpoqysLG3YsEH33HNPmb8P/H8GfjV//nwjyWzZsqXEdVwul2nZsqXn64ceesjExcWZ77//3rOsoKDAtGvXztx9992eZRMnTjSSzLJly4rcZ0FBgTHGmOnTpxtJZtGiRZ7rfvjhB/PAAw+Y0NBQ43a7jTHGrFixwkgyf/zjHz3rXblyxbRv395IMvPnz/d5vsLv/cEHHzRXrlwpdTsZY0zLli2Ny+W65nqFvv322yLL0tLSjMPhMMeOHfMs69ixo+nYsWORdQcNGmQaNGjg+XrkyJEmPDy81Fm///57k5+f77Xsq6++Mk6n07zwwgtey3663YqzZ88eExISYiSZFi1amJEjR5oVK1aYS5cuea2Xm5tr7rjjDjN06FCv5adOnTIul8tr+aRJk8xPf/Q7duxomjZtWuoshbZu3WokmdWrVxtjrv7bRkZGmpEjR3qt9+677xpJZt26dUXuo2nTpsVu88LnRGJiouc5aowxo0ePNoGBgebChQulzlb4vZ05c8aMHTvWNG7c2HNd69atzeDBg40xxkgyqampnuuuXLli8vLyvO7r/Pnzpk6dOmbIkCGeZWV5Dvz4Z/rkyZOmadOmpmHDhubo0aOlzo6SsUdVAYSGhnrO/jt37pzWrl2rRx99VLm5ucrOzlZ2drbOnj2rpKQkHTx4UCdOnJB09aWy5s2b65FHHilyn4WnJ//9739X3bp11bdvX891VatW1YgRI3Tx4kVt2LDBs16VKlX05JNPetYLDAzU8OHDve7Xl/kKDR06VIGBgdfcDm63W2FhYWXZZJLkdUzv0qVLys7OVrt27WSM0Y4dO8p8P4XuuOMOXbp0SatXry5xHafTqYCAqz9W+fn5Onv2rEJDQ3XPPfdo+/btPj9m06ZNtXPnTg0YMEBHjx7VjBkz1LNnT9WpU0fz5s3zrLd69WpduHBBffv29Wzz7OxsBQYGqm3btsW+hFVeGRkZqlOnjjp37izp6nPpscce05IlS5Sfn39DHuOJJ57wOoW+ffv2ys/P17Fjx8p8H/369dOhQ4e0ZcsWz3+Le9lPuvpcDgoKkiQVFBTo3LlzunLliuLj473+3cryHCh0/PhxdezYUZcvX9ZHH31U7Mu3KBtCVQFcvHjR8wv60KFDMsZowoQJql27ttel8GB24UH2w4cPq1mzZqXe97Fjx3T33Xd7frkWKnzpp/AXw7Fjx3TnnXcqNDTUa72fvozhy3yFYmJiyrQdwsPDfTpd/+uvv1ZKSopq1Kih0NBQ1a5dWx07dpQk5eTklPl+Cj311FP62c9+pm7duikyMlJDhgzRP/7xD691CgoK9Oqrr+ruu++W0+lUrVq1VLt2bX3++eflekxJ+tnPfqaFCxcqOztbn3/+uaZOnaoqVaroiSee0L/+9S9J0sGDByVdPa750+3+z3/+0+cTL86dO6dTp055LoWz5+fna8mSJercubO++uorHTp0SIcOHVLbtm2VlZWlNWvWlOt7/Kn69et7fV29enVJV4/1lFXLli0VGxurxYsXKyMjQ3Xr1lWXLl1KXH/BggW67777FBwcrJo1a6p27dr629/+5vXvVpbnQKGBAwfq9OnT2rBhg+66664yz42iOOvPcsePH1dOTo7n1N+CggJJ0tixY5WUlFTsbX56mvCtVJ75yno2Y2xsrHbs2KHMzExFRUWVum5+fr5+8Ytf6Ny5c3rmmWcUGxuratWq6cSJE0pJSfHMKV3dIzDGFHsfPxYREaGdO3fqww8/1KpVq7Rq1SrNnz9fjz/+uOfkk6lTp2rChAkaMmSIJk+erBo1aiggIECjRo3yeszyCAwMVFxcnOLi4vTAAw+oc+fOysjIUGJioue+Fy5cqLp16xa5bZUqvv2o9+rVy7M3LUmDBg3SW2+9pbVr1+rkyZNasmSJlixZUuR2GRkZ6tq1q4/fWVEl7WEX9+9Umn79+mn27NkKCwvTY489VuR/yAotWrRIKSkp6tmzp55++mlFREQoMDBQaWlpOnz4sGe9sjwHCvXq1Ut//vOfNWPGDKWlpfk0N7wRKsstXLhQkjy/9Bs2bCjp6stziYmJpd62UaNG2rNnT6nrNGjQQJ9//rkKCgq8fogLz5YqfLmiQYMGWrNmjS5evOi1V/XTM5h8mc9XPXr00Ntvv61FixZp/Pjxpa67e/duHThwQAsWLPA6eaK4l2yqV6+uI0eOFFle3MtMQUFB6tGjh3r06KGCggI99dRTmjt3riZMmKDGjRtr6dKl6ty5s958802v2124cEG1atUq67d6TYV/ynDy5ElJV/+tpau/SG/Edn/llVe89l7q1asn6WqIIiIi9NprrxW5zbJly7R8+XLNmTPH80e1JSntuhupX79+mjhxok6ePOn5WSrO0qVL1bBhQy1btsxrtuJOub/Wc6DQ8OHD1bhxY02cOFEul6vUE4VQOl76s9jatWs1efJkxcTEqH///pKu/iLq1KmT5s6d6/kl9WM/PoX317/+tXbt2lXsmXaF/2f68MMP69SpU3rnnXc81125ckUzZ85UaGio56Wyhx9+WFeuXNHs2bM96+Xn52vmzJle9+vLfL7q3bu34uLiNGXKFG3atKnI9bm5uZ6zxwr/j/zH/wdujCn2NOJGjRpp//79XrPt2rWryJlcZ8+e9fo6ICDA84fYeXl5nsf96f/1v/vuu0WOy5XVxx9/rMuXLxdZ/ve//13S/730mpSUpPDwcE2dOrXY9X3d7q1atVJiYqLn0qRJE3333XdatmyZunfvrt69exe5DBs2TLm5uZ6zMwv/Hq64P2GoVq1aiX/acCM1atRI06dPV1pamtq0aVPiesU9X/73f/+3yPOsLM+BH5swYYLGjh2r8ePHe/3swDfsUVli1apV2r9/v65cuaKsrCytXbtWq1evVoMGDbRy5UoFBwd71n3ttdf04IMPKi4uTkOHDlXDhg2VlZWlTZs26fjx456/2Xn66ae1dOlS9enTR0OGDFGrVq107tw5rVy5UnPmzFHz5s31xBNPaO7cuUpJSdG2bdsUHR2tpUuXauPGjZo+fbrn2FiPHj2UkJCgcePG6ejRo2rSpImWLVtW7HGXss7nq6pVq2rZsmVKTExUhw4d9OijjyohIUFVq1bV3r17tXjxYlWvXl1TpkxRbGysGjVqpLFjx+rEiRMKDw/Xe++9V+wxjiFDhmjatGlKSkrSb3/7W50+fVpz5sxR06ZN5Xa7Pev9+7//u86dO6cuXbooMjJSx44d08yZM9WiRQvPMb3u3bvrhRde0ODBg9WuXTvt3r1bGRkZnj1NX6Wnp2vbtm3q1auX5xfi9u3b9ec//1k1atTQqFGjJF09fjd79mwNHDhQ999/v37zm9+odu3a+vrrr/W3v/1NCQkJmjVrVrlmKLRy5Url5ubql7/8ZbHX//znP1ft2rWVkZGhxx57TC1atFBgYKDS09OVk5Mjp9OpLl26KCIiQq1atdLs2bP14osvqnHjxoqIiCj1+NH1GDly5DXX6d69u5YtW6ZHHnlEycnJ+uqrrzRnzhw1adJEFy9e9KxXlufAT7388svKyclRamqqwsLCNGDAgBv2vVUa/jrdEFcVnspaeAkKCjJ169Y1v/jFL8yMGTM8p4f/1OHDh83jjz9u6tata6pWrWruuusu0717d7N06VKv9c6ePWuGDRtm7rrrLhMUFGQiIyPNoEGDTHZ2tmedrKwsM3jwYFOrVi0TFBRk4uLiij1t+uzZs2bgwIEmPDzcuFwuM3DgQLNjx45iT7Muy3xlOTW/OOfPnzcTJ040cXFx5t/+7d9McHCwadasmRk/frw5efKkZ719+/aZxMREExoaamrVqmWGDh1qdu3aVey8ixYtMg0bNjRBQUGmRYsW5sMPPyxyevrSpUtN165dTUREhAkKCjL169c3v/vd77we8/vvvzf/+Z//ae68804TEhJiEhISzKZNm4qcAl/W09M3btxoUlNTTbNmzYzL5TJVq1Y19evXNykpKebw4cNF1l+3bp1JSkoyLpfLBAcHm0aNGpmUlBSzdetWzzrlPT29R48eJjg4uMip8T+WkpJiqlat6nl+zZs3zzRs2NAEBgZ6nap+6tQpk5ycbMLCwowkz7Yp6Tmxbt26Ek91/7Efn55eGv3k9PSCggIzdepU06BBA+N0Ok3Lli3NBx98UK7nQHHfQ35+vunbt6+pUqWKWbFiRamzoSiHMT4enQQA4BbiGBUAwGqECgBgNUIFALAaoQIAWI1QAQCsRqgAAFa75X/wW1BQoG+++UZhYWG37G1UAAB2McYoNzdX9erVK/E9GAvd8lB9880313xDUQBA5ZCZmVnqJ3tLfgiVL58n5A/l/SgGAEDZud1uRUVFlakJtzxUtr/cFx4e7u8RAKDSKEsTOJkCAGA1QgUAsBqhAgBYjVABAKxGqAAAViNUAACrESoAgNUIFQDAaoQKAGA1QgUAsBqhAgBYjVABAKxWrlC99tprio6OVnBwsNq2bavNmzff6LkAAJBUjlC98847GjNmjCZNmqTt27erefPmSkpK0unTp2/GfACASs5hjDG+3KBt27Zq3bq1Zs2aJenqJ/ZGRUVp+PDhGjdu3DVv73a75XK5yjftLeDj5gAAlENhC3Jycq758Uo+7VH98MMP2rZtmxITE//vDgIClJiYqE2bNpVvWgAASuHTBydmZ2crPz9fderU8Vpep04d7d+/v9jb5OXlKS8vz/O12+0ux5gAgMrqpp/1l5aWJpfL5blERUXd7IcEANxGfApVrVq1FBgYqKysLK/lWVlZqlu3brG3GT9+vHJycjyXzMzM8k8LAKh0fApVUFCQWrVqpTVr1niWFRQUaM2aNXrggQeKvY3T6VR4eLjXBQCAsvLpGJUkjRkzRoMGDVJ8fLzatGmj6dOn69KlSxo8ePDNmA8AUMn5HKrHHntMZ86c0cSJE3Xq1Cm1aNFC//jHP4qcYAEAwI3g899RXS/+jgoAcNP+jgoAgFuNUAEArEaoAABWI1QAAKsRKgCA1QgVAMBqhAoAYDVCBQCwGqECAFiNUAEArEaoAABWI1QAAKv5/O7ptzuHw+HvEUrEG+YCqIzYowIAWI1QAQCsRqgAAFYjVAAAqxEqAIDVCBUAwGqECgBgNUIFALAaoQIAWI1QAQCsRqgAAFYjVAAAqxEqAIDVCBUAwGqECgBgNUIFALAaoQIAWI1QAQCsRqgAAFYjVAAAqxEqAIDVCBUAwGqECgBgNUIFALAaoQIAWI1QAQCsRqgAAFYjVAAAqxEqAIDVCBUAwGqECgBgNUIFALAaoQIAWI1QAQCsRqgAAFYjVAAAqxEqAIDVCBUAwGpV/D0Ays7hcPh7hGIZY/w9AoDbGHtUAACrESoAgNUIFQDAaoQKAGA1QgUAsBqhAgBYjVABAKxGqAAAViNUAACrESoAgNUIFQDAaoQKAGA1QgUAsBqhAgBYzadQpaWlqXXr1goLC1NERIR69uypL7/88mbNBgCAb6HasGGDUlNT9dlnn2n16tW6fPmyunbtqkuXLt2s+QAAlZzDXMen3p05c0YRERHasGGDOnToUKbbuN1uuVyu8j4kLMQHJwLwVWELcnJyFB4eXuq613WMKicnR5JUo0aN67kbAABKVO6Poi8oKNCoUaOUkJCgZs2albheXl6e8vLyPF+73e7yPiQAoBIq9x5Vamqq9uzZoyVLlpS6Xlpamlwul+cSFRVV3ocEAFRC5TpGNWzYML3//vv66KOPFBMTU+q6xe1REavbC8eoAPjKl2NUPr30Z4zR8OHDtXz5cq1fv/6akZIkp9Mpp9Ppy8MAAODhU6hSU1O1ePFivf/++woLC9OpU6ckSS6XSyEhITdlQABA5ebTS38Oh6PY5fPnz1dKSkqZ7oPT028/vPQHwFc39aU/AABuJd7rDwBgNUIFALAaoQIAWI1QAQCsRqgAAFYjVAAAqxEqAIDVCBUAwGqECgBgNUIFALAaoQIAWI1QAQCsVu6PogcKlfSu+jbgjZSBio89KgCA1QgVAMBqhAoAYDVCBQCwGqECAFiNUAEArEaoAABWI1QAAKsRKgCA1QgVAMBqhAoAYDVCBQCwGqECAFiNUAEArEaoAABWI1QAAKsRKgCA1QgVAMBqhAoAYDVCBQCwGqECAFiNUAEArEaoAABWI1QAAKsRKgCA1QgVAMBqhAoAYDVCBQCwGqECAFiNUAEArEaoAABWI1QAAKsRKgCA1QgVAMBqhAoAYDVCBQCwGqECAFiNUAEArFbF3wMAN5PD4fD3CCUyxvh7BKBCYI8KAGA1QgUAsBqhAgBYjVABAKxGqAAAViNUAACrESoAgNUIFQDAaoQKAGA1QgUAsBqhAgBYjVABAKxGqAAAViNUAACrXVeoXnrpJTkcDo0aNeoGjQMAgLdyh2rLli2aO3eu7rvvvhs5DwAAXsoVqosXL6p///6aN2+eqlevfqNnAgDAo1yhSk1NVXJyshITE2/0PAAAePH5o+iXLFmi7du3a8uWLWVaPy8vT3l5eZ6v3W63rw8JAKjEfNqjyszM1MiRI5WRkaHg4OAy3SYtLU0ul8tziYqKKtegAIDKyWGMMWVdecWKFXrkkUcUGBjoWZafny+Hw6GAgADl5eV5XScVv0dFrADJhx894LbjdrvlcrmUk5Oj8PDwUtf16aW/hx56SLt37/ZaNnjwYMXGxuqZZ54pEilJcjqdcjqdvjwMAAAePoUqLCxMzZo181pWrVo11axZs8hyAABuBN6ZAgBgNZ/P+vup9evX34AxAAAoHntUAACrESoAgNUIFQDAaoQKAGA1QgUAsBqhAgBYjVABAKxGqAAAViNUAACrESoAgNUIFQDAaoQKAGC1635TWgDl43A4/D1CifhQR9iEPSoAgNUIFQDAaoQKAGA1QgUAsBqhAgBYjVABAKxGqAAAViNUAACrESoAgNUIFQDAaoQKAGA1QgUAsBqhAgBYjVABAKxGqAAAViNUAACrESoAgNUIFQDAaoQKAGA1QgUAsBqhAgBYjVABAKxGqAAAViNUAACrESoAgNUIFQDAaoQKAGA1QgUAsBqhAgBYjVABAKxGqAAAViNUAACrESoAgNUIFQDAaoQKAGA1QgUAsBqhAgBYjVABAKxWxd8DALCPw+Hw9wglMsb4ewTcYuxRAQCsRqgAAFYjVAAAqxEqAIDVCBUAwGqECgBgNUIFALAaoQIAWI1QAQCsRqgAAFYjVAAAqxEqAIDVCBUAwGqECgBgNZ9DdeLECQ0YMEA1a9ZUSEiI4uLitHXr1psxGwAAvn0e1fnz55WQkKDOnTtr1apVql27tg4ePKjq1avfrPkAAJWcT6FKT09XVFSU5s+f71kWExNzw4cCAKCQTy/9rVy5UvHx8erTp48iIiLUsmVLzZs372bNBgCAb6E6cuSIZs+erbvvvlsffvihnnzySY0YMUILFiwo8TZ5eXlyu91eFwAAysphjDFlXTkoKEjx8fH69NNPPctGjBihLVu2aNOmTcXe5rnnntPzzz9//ZMCgCQffmXBYm63Wy6XSzk5OQoPDy91XZ/2qO688041adLEa9m9996rr7/+usTbjB8/Xjk5OZ5LZmamLw8JAKjkfDqZIiEhQV9++aXXsgMHDqhBgwYl3sbpdMrpdJZvOgBApefTHtXo0aP12WefaerUqTp06JAWL16sN954Q6mpqTdrPgBAJefTMSpJ+uCDDzR+/HgdPHhQMTExGjNmjIYOHVrm2xe+LgkA5cExqtuDL8eofA7V9SJUAK4Hobo93LSTKQAAuNUIFQDAaoQKAGA1QgUAsBqhAgBYjVABAKxGqAAAViNUAACrESoAgNUIFQDAaoQKAGA1QgUAsJpPn0cFAP7mcDj8PUKxeLPcm4c9KgCA1QgVAMBqhAoAYDVCBQCwGqECAFiNUAEArEaoAABWI1QAAKsRKgCA1QgVAMBqhAoAYDVCBQCwGqECAFiNUAEArEaoAABWI1QAAKsRKgCA1QgVAMBqhAoAYDVCBQCwGqECAFiNUAEArEaoAABWI1QAAKsRKgCA1QgVAMBqhAoAYDVCBQCwGqECAFiNUAEArEaoAABWI1QAAKsRKgCA1QgVAMBqhAoAYDVCBQCwGqECAFiNUAEArFbF3wMAwO3A4XD4e4QSGWP8PcJ1YY8KAGA1QgUAsBqhAgBYjVABAKxGqAAAViNUAACrESoAgNUIFQDAaoQKAGA1QgUAsBqhAgBYjVABAKxGqAAAViNUAACr+RSq/Px8TZgwQTExMQoJCVGjRo00efLkCv8W8gAAe/n0eVTp6emaPXu2FixYoKZNm2rr1q0aPHiwXC6XRowYcbNmBABUYj6F6tNPP9WvfvUrJScnS5Kio6P19ttva/PmzTdlOAAAfHrpr127dlqzZo0OHDggSdq1a5c++eQTdevW7aYMBwCAT3tU48aNk9vtVmxsrAIDA5Wfn68pU6aof//+Jd4mLy9PeXl5nq/dbnf5pwUAVDo+7VH95S9/UUZGhhYvXqzt27drwYIF+q//+i8tWLCgxNukpaXJ5XJ5LlFRUdc9NACgEjE+iIyMNLNmzfJaNnnyZHPPPfeUeJvvv//e5OTkeC6ZmZlGEhcuXLhwuUUXG+Xk5BhJJicn55rr+vTS37fffquAAO+dsMDAQBUUFJR4G6fTKafT6cvDAADg4VOoevTooSlTpqh+/fpq2rSpduzYoWnTpmnIkCE3az4AQCXnMKbsf62bm5urCRMmaPny5Tp9+rTq1aunvn37auLEiQoKCirTfbjdbrlcrnIPDADwjQ+/5m+Zwhbk5OQoPDy81HV9CtWNQKgA4Naq6KHivf4AAFYjVAAAqxEqAIDVCBUAwGqECgBgNUIFALAaoQIAWI1QAQCsRqgAAFYjVAAAqxEqAIDVCBUAwGo+fcwHAKDicTgc/h7hurBHBQCwGqECAFiNUAEArEaoAABWI1QAAKsRKgCA1QgVAMBqhAoAYDVCBQCwGqECAFiNUAEArEaoAABWI1QAAKsRKgCA1QgVAMBqhAoAYDVCBQCwGqECAFiNUAEArEaoAABWI1QAAKsRKgCA1QgVAMBqhAoAYDVCBQCwGqECAFiNUAEArEaoAABWI1QAAKsRKgCA1QgVAMBqhAoAYDVCBQCwGqECAFiNUAEArEaoAABWI1QAAKvd8lAZY271QwIALFWWJtzyUOXm5t7qhwQAWKosTXCYW7yLU1BQoG+++UZhYWFyOBzXdV9ut1tRUVHKzMxUeHj4DZrw9sY2Kx+2m+/YZuVTWbabMUa5ubmqV6+eAgJK32eqcotm8ggICFBkZOQNvc/w8PDb+h/0ZmCblQ/bzXdss/KpDNvN5XKVaT1OpgAAWI1QAQCsVqFD5XQ6NWnSJDmdTn+PUmGwzcqH7eY7tln5sN2KuuUnUwAA4IsKvUcFALj9ESoAgNUIFQDAaoQKAGC1Chuq1157TdHR0QoODlbbtm21efNmf49ktbS0NLVu3VphYWGKiIhQz5499eWXX/p7rArlpZdeksPh0KhRo/w9ivVOnDihAQMGqGbNmgoJCVFcXJy2bt3q77Gslp+frwkTJigmJkYhISFq1KiRJk+ezPujqoKG6p133tGYMWM0adIkbd++Xc2bN1dSUpJOnz7t79GstWHDBqWmpuqzzz7T6tWrdfnyZXXt2lWXLl3y92gVwpYtWzR37lzdd999/h7FeufPn1dCQoKqVq2qVatWad++fXrllVdUvXp1f49mtfT0dM2ePVuzZs3SF198ofT0dP3xj3/UzJkz/T2a31XI09Pbtm2r1q1ba9asWZKuvn9gVFSUhg8frnHjxvl5uorhzJkzioiI0IYNG9ShQwd/j2O1ixcv6v7779frr7+uF198US1atND06dP9PZa1xo0bp40bN+rjjz/29ygVSvfu3VWnTh29+eabnmW//vWvFRISokWLFvlxMv+rcHtUP/zwg7Zt26bExETPsoCAACUmJmrTpk1+nKxiycnJkSTVqFHDz5PYLzU1VcnJyV7POZRs5cqVio+PV58+fRQREaGWLVtq3rx5/h7Leu3atdOaNWt04MABSdKuXbv0ySefqFu3bn6ezP9u+ZvSXq/s7Gzl5+erTp06Xsvr1Kmj/fv3+2mqiqWgoECjRo1SQkKCmjVr5u9xrLZkyRJt375dW7Zs8fcoFcaRI0c0e/ZsjRkzRn/4wx+0ZcsWjRgxQkFBQRo0aJC/x7PWuHHj5Ha7FRsbq8DAQOXn52vKlCnq37+/v0fzuwoXKly/1NRU7dmzR5988om/R7FaZmamRo4cqdWrVys4ONjf41QYBQUFio+P19SpUyVJLVu21J49ezRnzhxCVYq//OUvysjI0OLFi9W0aVPt3LlTo0aNUr169Sr9dqtwoapVq5YCAwOVlZXltTwrK0t169b101QVx7Bhw/TBBx/oo48+uuEft3K72bZtm06fPq3777/fsyw/P18fffSRZs2apby8PAUGBvpxQjvdeeedatKkideye++9V++9956fJqoYnn76aY0bN06/+c1vJElxcXE6duyY0tLSKn2oKtwxqqCgILVq1Upr1qzxLCsoKNCaNWv0wAMP+HEyuxljNGzYMC1fvlxr165VTEyMv0ey3kMPPaTdu3dr586dnkt8fLz69++vnTt3EqkSJCQkFPnThwMHDqhBgwZ+mqhi+Pbbb4t8gGBgYKAKCgr8NJE9KtwelSSNGTNGgwYNUnx8vNq0aaPp06fr0qVLGjx4sL9Hs1ZqaqoWL16s999/X2FhYTp16pSkqx9cFhIS4ufp7BQWFlbkGF61atVUs2ZNju2VYvTo0WrXrp2mTp2qRx99VJs3b9Ybb7yhN954w9+jWa1Hjx6aMmWK6tevr6ZNm2rHjh2aNm2ahgwZ4u/R/M9UUDNnzjT169c3QUFBpk2bNuazzz7z90hWk1TsZf78+f4erULp2LGjGTlypL/HsN5f//pX06xZM+N0Ok1sbKx54403/D2S9dxutxk5cqSpX7++CQ4ONg0bNjTPPvusycvL8/doflch/44KAFB5VLhjVACAyoVQAQCsRqgAAFYjVAAAqxEqAIDVCBUAwGqECgBgNUIFALAaoQIAWI1QAQCsRqgAAFYjVAAAq/0/BsKyCrGC8KEAAAAASUVORK5CYII="},"metadata":{}}],"execution_count":58},{"cell_type":"markdown","source":"# Transformer Decoder Components\n\n\n\nWe will use these components in the Transformer decoder. These include positional encoding, feed-forward networks, scaled dot-product attention, and multi-head attention. Each of these components plays a vital role in processing input sequences and computing attention in the Transformer model.\n\n\n\n---\n\n\n\n## 1. **Positional Encoding (`PositionalEncoding`)**\n\nTransformers do not inherently capture the order of sequences, so positional encodings are used to introduce sequence order into the model.\n\n\n\n- **Purpose**: Adds information about the position of each token in the input sequence.\n\n- **Mechanism**: Uses a combination of sine and cosine functions of different frequencies to generate positional encodings.\n\n- **Parameters**:\n\n  - `projection_size`: The size of the input embeddings (i.e., `d_model`).\n\n  - `max_seq_len`: The maximum length of the input sequence (default: 1000).\n\n- **Output**: The input embedding enriched with positional information, which is passed through a dropout layer for regularization.\n\n\n\n---","metadata":{"id":"j1GobVvxYdy2"}},{"cell_type":"code","source":"class PositionalEncoding(torch.nn.Module):\n\n\n\n    def __init__(self, projection_size, max_seq_len= 1000, dropout=0.1):\n\n        super().__init__()\n\n        self.dropout                = torch.nn.Dropout(dropout)\n\n\n\n        pe              = torch.zeros(max_seq_len, projection_size)\n\n        position        = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n\n        div_term        = torch.exp(torch.arange(0, projection_size, 2).float() * (-math.log(10000.0) / projection_size))\n\n        pe[:, 0::2]     = torch.sin(position * div_term)\n\n        pe[:, 1::2]     = torch.cos(position * div_term)\n\n        pe              = pe.unsqueeze(0)\n\n        self.register_buffer('pe', pe)\n\n\n\n    def forward(self, x):\n\n        return self.dropout(x + self.pe[:, :x.size(1)])\n\n\n\n","metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1732104810641,"user":{"displayName":"Pierre Ntakirutimana","userId":"14527179770260799179"},"user_tz":-120},"id":"wixQHpDaYdy2","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:54:19.059861Z","iopub.execute_input":"2024-11-20T15:54:19.060217Z","iopub.status.idle":"2024-11-20T15:54:19.066563Z","shell.execute_reply.started":"2024-11-20T15:54:19.060179Z","shell.execute_reply":"2024-11-20T15:54:19.065706Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"# import torch\n\n# import math\n\n# import torch.nn as nn\n\n\n\n# class PositionalEncoding(nn.Module):\n\n#     def __init__(self, projection_size, max_seq_len, dropout):\n\n#         \"\"\"\n\n#         Positional Encoding for adding positional information to token embeddings.\n\n\n\n#         Args:\n\n#             projection_size (int): Dimension of the input embeddings.\n\n#             max_seq_len (int): Maximum sequence length.\n\n#             dropout (float): Dropout probability.\n\n#         \"\"\"\n\n#         super(PositionalEncoding, self).__init__()\n\n#         self.dropout = nn.Dropout(p=dropout)\n\n\n\n#         # Create a matrix to hold positional encodings\n\n#         pe = torch.zeros(max_seq_len, projection_size)\n\n#         position = torch.arange(0, max_seq_len).unsqueeze(1).float()\n\n\n\n#         # Handle odd `projection_size`\n\n#         div_term = torch.exp(torch.arange(0, projection_size, 2).float() * (-math.log(10000.0) / projection_size))\n\n#         pe[:, 0::2] = torch.sin(position * div_term)  # Even indices\n\n#         if projection_size % 2 == 0:\n\n#             pe[:, 1::2] = torch.cos(position * div_term)  # Odd indices\n\n#         else:\n\n#             pe[:, 1::2] = torch.cos(position * div_term[:projection_size // 2])  # Truncate if odd\n\n\n\n#         pe = pe.unsqueeze(0)  # Add batch dimension\n\n#         self.register_buffer('pe', pe)\n\n\n\n#     def forward(self, x):\n\n#         \"\"\"\n\n#         Adds positional encoding to the input tensor.\n\n\n\n#         Args:\n\n#             x: Input tensor of shape (batch_size, seq_len, projection_size).\n\n\n\n#         Returns:\n\n#             Tensor with positional encodings added, same shape as input.\n\n#         \"\"\"\n\n#         x = x + self.pe[:, :x.size(1), :]  # Match sequence length\n\n#         return self.dropout(x)","metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1732104810643,"user":{"displayName":"Pierre Ntakirutimana","userId":"14527179770260799179"},"user_tz":-120},"id":"PRS8AheZSZ18","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:54:19.067554Z","iopub.execute_input":"2024-11-20T15:54:19.067797Z","iopub.status.idle":"2024-11-20T15:54:19.081587Z","shell.execute_reply.started":"2024-11-20T15:54:19.067773Z","shell.execute_reply":"2024-11-20T15:54:19.080742Z"}},"outputs":[],"execution_count":60},{"cell_type":"markdown","source":"\n\n## 2. **Feed-Forward Network (`FeedForward`)**\n\nThe feed-forward network is a fully connected layer applied independently to each position in the sequence after the attention layers.\n\n\n\n- **Purpose**: Projects the intermediate representations to a higher-dimensional space and back to the original model dimension.\n\n- **Mechanism**: Consists of two linear layers with a GeLU activation function and dropout in between.\n\n- **Parameters**:\n\n  - `d_model`: The input and output dimensionality of the model.\n\n  - `d_ff`: The dimensionality of the hidden layer in the feed-forward network (default: 2048).\n\n  - `dropout`: Dropout rate applied after the GeLU activation (default: 0.1).\n\n- **Output**: The transformed input sequence passed through two linear transformations with non-linear activation in between.\n\n\n\n---","metadata":{"id":"DUt8AjEEYdy2"}},{"cell_type":"code","source":"\n\nclass FeedForward(torch.nn.Module):\n\n    ''' Projection Layer (Fully Connected Layers) '''\n\n\n\n    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n\n        super().__init__()\n\n\n\n        self.linear_1   = torch.nn.Linear(d_model, d_ff)\n\n        self.dropout    = torch.nn.Dropout(dropout)\n\n        self.linear_2   = torch.nn.Linear(d_ff, d_model)\n\n\n\n    def forward(self, x):\n\n\n\n        # Apply the first linear layer, GeLU activation, and then dropout\n\n        x = self.dropout(torch.nn.functional.gelu(self.linear_1(x)))\n\n\n\n        # Apply the second linear layer to project the dimension back to d_model\n\n        x = self.linear_2(x)\n\n\n\n        return x","metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1732104811213,"user":{"displayName":"Pierre Ntakirutimana","userId":"14527179770260799179"},"user_tz":-120},"id":"f5UzjJmRYdy2","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:54:19.082687Z","iopub.execute_input":"2024-11-20T15:54:19.083226Z","iopub.status.idle":"2024-11-20T15:54:19.092108Z","shell.execute_reply.started":"2024-11-20T15:54:19.083198Z","shell.execute_reply":"2024-11-20T15:54:19.091205Z"}},"outputs":[],"execution_count":61},{"cell_type":"markdown","source":"\n\n\n\n## 3. **Scaled Dot-Product Attention (`ScaledDotProductAttention`)**\n\nThis module computes the attention score for each query-key pair in the input sequence using the scaled dot-product mechanism.\n\n\n\n- **Purpose**: To compute attention scores and generate weighted outputs based on the input query, key, and value matrices.\n\n- **Mechanism**:\n\n  - Calculates the dot product of queries and keys, scales by the square root of the dimension, and applies a softmax to generate attention weights.\n\n  - Uses dropout for regularization.\n\n- **Parameters**:\n\n  - `temperature`: Scaling factor for the dot product.\n\n  - `attn_dropout`: Dropout rate for attention weights (default: 0.1).\n\n- **Output**: Returns the weighted sum of the values and the attention weights.\n\n\n\n---\n\n","metadata":{"id":"vofvMG5lYdy7"}},{"cell_type":"code","source":"class ScaledDotProductAttention(torch.nn.Module):\n\n    ''' Scaled Dot-Product Attention '''\n\n\n\n    def __init__(self, temperature, attn_dropout=0.1):\n\n        super().__init__()\n\n        self.temperature    = temperature                       # Scaling factor for the dot product\n\n        self.dropout        = torch.nn.Dropout(attn_dropout)    # Dropout layer for attention weights\n\n        self.softmax        = torch.nn.Softmax(dim=-1)           # Softmax layer along the attention dimension\n\n\n\n    def forward(self, q, k, v, mask=None):\n\n\n\n        # Calculate the dot product between queries and keys.\n\n        # attn = torch.bmm(q, k.transpose(1, 2))\n\n        attn = (q @ k.transpose(-2, -1))\n\n\n\n        # Scale the dot product by the temperature.\n\n        attn = attn / self.temperature\n\n\n\n        if mask is not None:\n\n            # Apply the mask by setting masked positions to a large negative value.\n\n            # This ensures they have a softmax score close to zero.\n\n            attn = attn.masked_fill(mask, float('-inf'))\n\n\n\n        # Apply softmax to obtain attention weights.\n\n        attn    = self.softmax(attn)\n\n\n\n        # Apply dropout to the attention weights.\n\n        # Compute the weighted sum of values based on the attention weights.\n\n        # output  = torch.bmm(self.dropout(attn), v)\n\n        attn = self.dropout(attn)\n\n        output = attn @ v\n\n\n\n        return output, attn # Return the attention output and the attention weights.","metadata":{"executionInfo":{"elapsed":555,"status":"ok","timestamp":1732104813580,"user":{"displayName":"Pierre Ntakirutimana","userId":"14527179770260799179"},"user_tz":-120},"id":"wMB9Jb45Ydy7","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:54:19.093298Z","iopub.execute_input":"2024-11-20T15:54:19.093643Z","iopub.status.idle":"2024-11-20T15:54:19.103403Z","shell.execute_reply.started":"2024-11-20T15:54:19.093618Z","shell.execute_reply":"2024-11-20T15:54:19.102595Z"}},"outputs":[],"execution_count":62},{"cell_type":"markdown","source":"## 4. **Multi-Head Attention (`MultiHeadAttention`)**\n\nThis module implements multi-head attention, where multiple sets of attention heads are computed in parallel, and their outputs are concatenated.\n\n\n\n- **Purpose**: To allow the model to jointly attend to different positions in the input sequence from different representation subspaces.\n\n- **Mechanism**:\n\n  - Projects the input query, key, and value matrices into multiple smaller subspaces (heads).\n\n  - Computes scaled dot-product attention for each head in parallel.\n\n  - Concatenates the outputs of all heads and applies a final linear transformation to project the result back to the original model dimension.\n\n- **Parameters**:\n\n  - `n_head`: Number of attention heads.\n\n  - `d_model`: Dimensionality of the input and output representations.\n\n  - `dropout`: Dropout rate applied to the attention output (default: 0.1).\n\n- **Output**: Returns the concatenated output of all attention heads and the averaged attention weights.\n\n\n\n---","metadata":{"id":"KjSRj_hjYdy7"}},{"cell_type":"code","source":"class MultiHeadAttention(torch.nn.Module):\n\n    ''' Multi-Head Attention Module '''\n\n\n\n    def __init__(self, n_head, d_model, dropout=0.1):\n\n        super().__init__()\n\n\n\n        self.n_head = n_head # Number of attention heads\n\n        self.d_k    = d_model // n_head\n\n        self.d_v    = d_model // n_head\n\n\n\n\n\n        # Linear layers for projecting the input query, key, and value to multiple heads\n\n        self.w_qs   = torch.nn.Linear(d_model, n_head * self.d_k)\n\n        self.w_ks   = torch.nn.Linear(d_model, n_head * self.d_k)\n\n        self.w_vs   = torch.nn.Linear(d_model, n_head * self.d_v)\n\n\n\n        torch.nn.init.normal_(self.w_qs.weight, mean=0, std=np.sqrt(2.0 / (d_model + self.d_k)))\n\n        torch.nn.init.normal_(self.w_ks.weight, mean=0, std=np.sqrt(2.0 / (d_model + self.d_k)))\n\n        torch.nn.init.normal_(self.w_vs.weight, mean=0, std=np.sqrt(2.0 / (d_model + self.d_v)))\n\n\n\n        # Initialize the weights of the linear layers\n\n        self.attention = ScaledDotProductAttention(\n\n            temperature=np.power(self.d_k, 0.5), attn_dropout=dropout)\n\n\n\n        # Final linear layer to project the concatenated outputs of the attention heads back to the model dimension\n\n        self.fc = torch.nn.Linear(n_head * self.d_v, d_model)\n\n        torch.nn.init.normal_(self.fc.weight)\n\n\n\n        self.dropout = torch.nn.Dropout(dropout)\n\n\n\n    def forward(self, q, k, v, mask=None):\n\n\n\n        # following key, value, query standard computation\n\n        d_k, d_v, n_head    = self.d_k, self.d_v, self.n_head\n\n        sz_b, len_q, _      = q.size()\n\n        sz_b, len_k, _      = k.size()\n\n        sz_b, len_v, _      = v.size()\n\n\n\n        # Project the input query, key, and value to multiple heads\n\n        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)\n\n        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)\n\n        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)\n\n\n\n        # Rearrange the dimensions to group the heads together for parallel processing\n\n        q = q.transpose(1, 2)\n\n        k = k.transpose(1, 2)\n\n        v = v.transpose(1, 2)\n\n\n\n\n\n        # Repeat the mask for each attention head if a mask is provided\n\n        if mask is not None:\n\n              # print(mask.shape)\n\n              mask = mask.unsqueeze(1).repeat(1, n_head, 1, 1)\n\n\n\n        # Apply scaled dot-product attention to the projected query, key, and value\n\n        output, attn    = self.attention(q, k, v, mask=mask)\n\n\n\n        # Rearrange the output back to the original order and concatenate the heads\n\n        output = output.transpose(1, 2).contiguous().view(sz_b, len_v, -1)\n\n\n\n        output          = self.dropout(self.fc(output))\n\n\n\n        attn_weights = attn.mean(dim=(0, 1))\n\n\n\n        return output, attn_weights","metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1732104814243,"user":{"displayName":"Pierre Ntakirutimana","userId":"14527179770260799179"},"user_tz":-120},"id":"TRa-PPXrYdy7","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:54:19.104504Z","iopub.execute_input":"2024-11-20T15:54:19.104791Z","iopub.status.idle":"2024-11-20T15:54:19.116709Z","shell.execute_reply.started":"2024-11-20T15:54:19.104752Z","shell.execute_reply":"2024-11-20T15:54:19.115936Z"}},"outputs":[],"execution_count":63},{"cell_type":"markdown","source":"# Transformer Decoder Layers\n\n\n\nThe `DecoderLayer1` and `DecoderLayer3` are modular components of the Transformer decoder. Each layer is designed to handle a specific function: self-attention, cross-attention, and feed-forward processing.\n\n\n\n## 1. `DecoderLayer1`: Self-Attention Layer\n\n- **Purpose**: Implements self-attention, where the decoder attends to its own inputs, combined with residual connections and layer normalization.\n\n- **Components**:\n\n  - `MultiHeadAttention`: Applies self-attention to the target sequence.\n\n  - `LayerNorm`: Normalizes the output after the residual connection.\n\n  - `Dropout`: Regularization to prevent overfitting.\n\n\n\n## 3. `DecoderLayer3`: Feed-Forward Layer\n\n- **Purpose**: Implements a feed-forward neural network for further transformation of the decoder's intermediate representations.\n\n- **Components**:\n\n  - `FeedForward`: A two-layer fully connected network with non-linearity.\n\n  - `LayerNorm`: Applied after the residual connection.\n\n  - `Dropout`: Regularization to avoid overfitting.\n\n","metadata":{"id":"Rlvl41QtYdy7"}},{"cell_type":"code","source":"class DecoderLayer1(nn.Module):\n\n    def __init__(self, d_model, num_heads, d_ff, dropout):\n\n        \"\"\"\n\n        DecoderLayer (attention and layer norm) in the Transformer architecture.\n\n\n\n        Args:\n\n            d_model (int): The number of expected features in the input (embedding dimension).\n\n            num_heads (int): Number of attention heads.\n\n            d_ff (int): Dimension of the feedforward network model.\n\n            dropout (float): Dropout probability.\n\n        \"\"\"\n\n        super(DecoderLayer1, self).__init__()\n\n\n\n        # TODO: fill in the blanks appropriately (given the modules above)\n\n        self.self_attn = MultiHeadAttention(d_model = d_model, n_head = num_heads, dropout = dropout) # NotImplemented\n\n        self.layer_norm = nn.LayerNorm(d_model)   # NotImplemented\n\n        self.dropout = nn.Dropout(dropout) #     NotImplemented\n\n\n\n    def forward(self, tgt, attn_mask=None, key_padding_mask=None):\n\n        # TODO: apply layer norm to input\n\n        tgt_norm = self.layer_norm(tgt)\n\n\n\n        # TODO: call self attention with mask\n\n        tgt_att, attn_weights  = self.self_attn.forward(tgt_norm, tgt_norm, tgt_norm,\n\n                                                mask=attn_mask,\n\n                                                # key_padding_mask=key_padding_mask\n\n                                                        )\n\n\n\n        # TODO: apply dropout\n\n        tgt_att = self.dropout(tgt_att)\n\n\n\n        # TODO: add skip connection\n\n        tgt = tgt + tgt_att\n\n\n\n\n\n\n\n        # raise NotImplemented\n\n        return tgt, attn_weights\n\n\n\n\n\nclass DecoderLayer3(nn.Module):\n\n    def __init__(self, d_model, num_heads, d_ff, dropout):\n\n        \"\"\"\n\n        Feedforward layer with layer normalization in the Transformer decoder.\n\n\n\n        Args:\n\n            d_model (int): Embedding dimension.\n\n            num_heads (int): Number of attention heads.\n\n            d_ff (int): Dimension of the feedforward network.\n\n            dropout (float): Dropout probability.\n\n        \"\"\"\n\n        super(DecoderLayer3, self).__init__()\n\n\n\n        # TODO: fill in the blanks appropriately (given the modules above)\n\n        self.ffn =  FeedForward(d_model = d_model, d_ff = d_ff, dropout = dropout) #NotImplemented\n\n        self.layer_norm = nn.LayerNorm(d_model)\n\n        self.dropout =  nn.Dropout(dropout) #NotImplemented\n\n\n\n    def forward(self, tgt):\n\n        # TODO: apply layer norm to input\n\n        tgt_norm = self.layer_norm(tgt)\n\n\n\n        # TODO: call feed forward layer\n\n        tgt_ffn = self.ffn(tgt_norm)\n\n\n\n        # TODO: apply dropout\n\n        tgt_ffn = self.dropout(tgt_ffn)\n\n\n\n        # TODO: add skip connection\n\n        tgt = tgt + tgt_ffn\n\n\n\n\n\n        # raise NotImplemented\n\n        return tgt","metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1732104815446,"user":{"displayName":"Pierre Ntakirutimana","userId":"14527179770260799179"},"user_tz":-120},"id":"5hX2kBrAYdy7","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:54:19.117669Z","iopub.execute_input":"2024-11-20T15:54:19.117910Z","iopub.status.idle":"2024-11-20T15:54:19.131242Z","shell.execute_reply.started":"2024-11-20T15:54:19.117887Z","shell.execute_reply":"2024-11-20T15:54:19.130587Z"}},"outputs":[],"execution_count":64},{"cell_type":"markdown","source":"# Causal Language Model\n\n\n\nThis module implements a Transformer-based decoder for causal language modeling (CLM). It consists of several components, including embedding layers, positional encoding, self-attention layers, and feed-forward layers. It supports various generation strategies such as beam search and sampling.\n\n\n\n### Key Components:\n\n- **Embedding Layer**: Converts input tokens into dense vector representations.\n\n- **Positional Encoding**: Adds position information to input tokens, helping the model understand the order of tokens.\n\n- **Decoder Layers**: Composed of:\n\n  - `DecoderLayer1`: Implements self-attention and layer normalization.\n\n  - `DecoderLayer3`: Implements a feed-forward network with residual connections.\n\n- **Output Linear Layer**: Projects the hidden states to the vocabulary size to generate output probabilities.\n\n\n\n### Key Methods:\n\n- **`forward`**: Runs the input through the decoder layers and generates output probabilities.","metadata":{"id":"cnMbHHLcYdy7"}},{"cell_type":"code","source":"class CausalLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size=64, d_model=256, num_layers=2, num_heads=2, d_ff=512, dropout=0.1, max_length=1000):\n\n\n\n        \"\"\"\n\n        Decoder module in the Transformer architecture.\n\n        Initializes embeddings, multiple decoder layers, and an output linear layer.\n\n\n\n        Args:\n\n            vocab_size (int): Size of the vocabulary.\n\n            d_model (int): The number of expected features in the input (embedding dimension).\n\n            num_layers (int): Number of decoder layers.\n\n            num_heads (int): Number of attention heads.\n\n            d_ff (int): Dimension of the feedforward network model.\n\n            dropout (float): Dropout probability.\n\n            max_length (int): Maximum length of input sequences.\n\n        \"\"\"\n\n        super(CausalLanguageModel, self).__init__()\n\n\n\n        # TODO: fill in the blanks appropriately (given the modules above)\n\n\n\n        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)   # NotImplemented\n\n\n\n        self.pos_encoder = PositionalEncoding(projection_size = d_model, max_seq_len= max_length, dropout=dropout) #NotImplemented\n\n        self.num_layers= num_layers # NotImplemented\n\n        self.dec_layers1 = DecoderLayer1(d_model = d_model, num_heads = num_heads , d_ff = d_ff, dropout = dropout) #NotImplemented\n\n        self.dec_layers3 = DecoderLayer3(d_model = d_model, num_heads = num_heads, d_ff = d_ff, dropout = dropout) #NotImplemented\n        self.layer_nrom =nn.LayerNorm(d_model, eps=1e-6) #NotImplemented\n\n        self.fully_connected = nn.Linear(d_model, vocab_size) #NotImplemented\n\n        self.apply(self._init_weights)\n\n\n\n    def _init_weights(self, module):\n\n        if isinstance(module, nn.Linear):\n\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n            if module.bias is not None:\n\n                torch.nn.init.zeros_(module.bias)\n\n        elif isinstance(module, nn.Embedding):\n\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n\n\n\n\n    def forward(self, inp):\n\n        # TODO: generate the causal mask using the given function\n\n        attn_mask = create_mask(inp) # NotImplemented\n\n\n\n        # TODO: convert input to embeddings\n\n        embeddings = self.embedding(inp)\n\n\n\n\n\n\n\n        # TODO: apply positional encoding\n\n        inp = self.pos_encoder(embeddings)\n\n\n\n        attention_weights_list = []\n\n\n\n        for i in range(self.num_layers):\n\n            # TODO: apply decoder layer\n\n            inp, attn_weights = self.dec_layers1.forward(inp, attn_mask) #NotImplemented\n\n            inp = self.dec_layers3.forward(inp) #NotImplemented\n\n            attention_weights_list.append(attn_weights)\n\n\n\n        # TODO: apply layernorm and the fully connected layer for classification\n\n        output = self.layer_nrom(inp)  #NotImplemented\n\n        output = self.fully_connected(output) #NotImplemented\n\n\n\n        stacked_attention_weights = torch.stack(attention_weights_list, dim=0)\n\n\n\n        return output, stacked_attention_weights","metadata":{"executionInfo":{"elapsed":762,"status":"ok","timestamp":1732105274574,"user":{"displayName":"Pierre Ntakirutimana","userId":"14527179770260799179"},"user_tz":-120},"id":"cebwoorWttWe","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:54:19.132506Z","iopub.execute_input":"2024-11-20T15:54:19.132828Z","iopub.status.idle":"2024-11-20T15:54:19.145769Z","shell.execute_reply.started":"2024-11-20T15:54:19.132793Z","shell.execute_reply":"2024-11-20T15:54:19.145067Z"}},"outputs":[],"execution_count":65},{"cell_type":"markdown","source":"# Model, Loss, Optimizer, and Scheduler Definition","metadata":{"id":"DelhoytAQWQa"}},{"cell_type":"code","source":"config = {\n    \"token_type\": \"10k\",  # Use a larger vocabulary for more granular modeling\n    \"d_model\": 512,  # Increase the embedding dimension for richer token representation\n    \"num_layers\": 5,  # Add more layers to improve model capacity\n    \"num_heads\": 2,  # Match the industry standard (e.g., GPT-2) for improved attention granularity\n    \"d_ff\": 2048,  # Increase feed-forward network size for better representation learning\n    \"dropout\": 0.2,  # Increase dropout to reduce overfitting\n    \"max_length\": 20,  # Keep the sequence length consistent\n    \"lr\": 2e-4,  # Reduce learning rate for more stable convergence\n    \"batch_size\": 64,  # Decrease batch size to allow for deeper models with limited GPU memory\n    \"num_epochs\": 15,  # Train longer to allow the model to converge better\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:54:19.146718Z","iopub.execute_input":"2024-11-20T15:54:19.146953Z","iopub.status.idle":"2024-11-20T15:54:19.159426Z","shell.execute_reply.started":"2024-11-20T15:54:19.146930Z","shell.execute_reply":"2024-11-20T15:54:19.158820Z"},"id":"0uv39vXrcASy","executionInfo":{"status":"ok","timestamp":1732105274574,"user_tz":-120,"elapsed":2,"user":{"displayName":"Pierre Ntakirutimana","userId":"14527179770260799179"}}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"# TODO: Define the model vocab_size=31, d_model=256, num_layers=2, num_heads=2, d_ff=512, dropout=0.1, max_length=1000\n\nimport torch.nn as nn\n\nmodel = CausalLanguageModel(\n\n    vocab_size = VOCAB_SIZE,\n\n    d_model    = config['d_model'],\n\n    num_layers = config['num_layers'],\n\n    num_heads  = config['num_heads'],\n\n    d_ff       = config['d_ff'],\n\n    dropout    = config['dropout'],\n\n    max_length = config['max_length']\n\n).to(DEVICE)\n\n\n\n# TODO: Define the dataloader\n\ntrain_loader = DataLoaderForLanguageModeling(\n\n    dataset=train_dataset,\n\n    batch_size=config['batch_size'],\n\n    sequence_length=3,\n\n    shuffle=True,\n\n    drop_last=True\n\n)\n\n\n\nval_loader = DataLoaderForLanguageModeling(\n\n    dataset=val_dataset,\n\n    batch_size=config['batch_size'],\n\n    sequence_length=3,\n\n    shuffle=True,\n\n    drop_last=True\n\n)\n\n\n\n# TODO: Define the criterion\n\ncriterion = nn.CrossEntropyLoss() #None\n\n\n\n# TODO: Define the optimizer\n\noptimizer = torch.optim.Adam(model.parameters(), lr=config['lr']) # None\n\n\n\n# TODO: Define the learning rate scheduler\n\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95) #None\n\n\n\n# Optional TODO: Define the scaler for mixed precision training\n\nscaler = torch.cuda.amp.GradScaler() # None\n\n\n\n# Print the model architecture and parameter summary\n\nprint(model)\n\n\n\n# Optionally, if you want to summarize the model, make sure `torchsummaryX` is installed\n\nsummary = torchsummaryX.summary(model.to(DEVICE), x=torch.tensor(inputs).to(DEVICE))\n\n","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1732105276890,"user":{"displayName":"Pierre Ntakirutimana","userId":"14527179770260799179"},"user_tz":-120},"id":"DbHH6zXTSwRa","outputId":"d7e83ebf-097d-48fa-93f2-4397c92e8699","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:54:19.160432Z","iopub.execute_input":"2024-11-20T15:54:19.160954Z","iopub.status.idle":"2024-11-20T15:54:19.408344Z","shell.execute_reply.started":"2024-11-20T15:54:19.160912Z","shell.execute_reply":"2024-11-20T15:54:19.407445Z"},"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"name":"stdout","text":"CausalLanguageModel(\n  (embedding): Embedding(10000, 512)\n  (pos_encoder): PositionalEncoding(\n    (dropout): Dropout(p=0.2, inplace=False)\n  )\n  (dec_layers1): DecoderLayer1(\n    (self_attn): MultiHeadAttention(\n      (w_qs): Linear(in_features=512, out_features=512, bias=True)\n      (w_ks): Linear(in_features=512, out_features=512, bias=True)\n      (w_vs): Linear(in_features=512, out_features=512, bias=True)\n      (attention): ScaledDotProductAttention(\n        (dropout): Dropout(p=0.2, inplace=False)\n        (softmax): Softmax(dim=-1)\n      )\n      (fc): Linear(in_features=512, out_features=512, bias=True)\n      (dropout): Dropout(p=0.2, inplace=False)\n    )\n    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0.2, inplace=False)\n  )\n  (dec_layers3): DecoderLayer3(\n    (ffn): FeedForward(\n      (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n      (dropout): Dropout(p=0.2, inplace=False)\n      (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n    )\n    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0.2, inplace=False)\n  )\n  (layer_nrom): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n  (fully_connected): Linear(in_features=512, out_features=10000, bias=True)\n)\n----------------------------------------------------------------------------------------------------\nLayer                   Kernel Shape         Output Shape         # Params (K)      # Mult-Adds (M)\n====================================================================================================\n0_Embedding             [512, 10000]         [64, 3, 512]             5,120.00                 5.12\n1_Dropout                          -         [64, 3, 512]                    -                    -\n2_LayerNorm                    [512]         [64, 3, 512]                 1.02                 0.00\n3_Linear                  [512, 512]         [64, 3, 512]               262.66                 0.26\n4_Linear                  [512, 512]         [64, 3, 512]               262.66                 0.26\n5_Linear                  [512, 512]         [64, 3, 512]               262.66                 0.26\n6_Softmax                          -        [64, 2, 3, 3]                    -                    -\n7_Dropout                          -        [64, 2, 3, 3]                    -                    -\n8_Linear                  [512, 512]         [64, 3, 512]               262.66                 0.26\n9_Dropout                          -         [64, 3, 512]                    -                    -\n10_Dropout                         -         [64, 3, 512]                    -                    -\n11_LayerNorm                   [512]         [64, 3, 512]                 1.02                 0.00\n12_Linear                [512, 2048]        [64, 3, 2048]             1,050.62                 1.05\n13_Dropout                         -        [64, 3, 2048]                    -                    -\n14_Linear                [2048, 512]         [64, 3, 512]             1,049.09                 1.05\n15_Dropout                         -         [64, 3, 512]                    -                    -\n16_LayerNorm                   [512]         [64, 3, 512]          (recursive)                 0.00\n17_Linear                 [512, 512]         [64, 3, 512]          (recursive)                 0.26\n18_Linear                 [512, 512]         [64, 3, 512]          (recursive)                 0.26\n19_Linear                 [512, 512]         [64, 3, 512]          (recursive)                 0.26\n20_Softmax                         -        [64, 2, 3, 3]                    -                    -\n21_Dropout                         -        [64, 2, 3, 3]                    -                    -\n22_Linear                 [512, 512]         [64, 3, 512]          (recursive)                 0.26\n23_Dropout                         -         [64, 3, 512]                    -                    -\n24_Dropout                         -         [64, 3, 512]                    -                    -\n25_LayerNorm                   [512]         [64, 3, 512]          (recursive)                 0.00\n26_Linear                [512, 2048]        [64, 3, 2048]          (recursive)                 1.05\n27_Dropout                         -        [64, 3, 2048]                    -                    -\n28_Linear                [2048, 512]         [64, 3, 512]          (recursive)                 1.05\n29_Dropout                         -         [64, 3, 512]                    -                    -\n30_LayerNorm                   [512]         [64, 3, 512]          (recursive)                 0.00\n31_Linear                 [512, 512]         [64, 3, 512]          (recursive)                 0.26\n32_Linear                 [512, 512]         [64, 3, 512]          (recursive)                 0.26\n33_Linear                 [512, 512]         [64, 3, 512]          (recursive)                 0.26\n34_Softmax                         -        [64, 2, 3, 3]                    -                    -\n35_Dropout                         -        [64, 2, 3, 3]                    -                    -\n36_Linear                 [512, 512]         [64, 3, 512]          (recursive)                 0.26\n37_Dropout                         -         [64, 3, 512]                    -                    -\n38_Dropout                         -         [64, 3, 512]                    -                    -\n39_LayerNorm                   [512]         [64, 3, 512]          (recursive)                 0.00\n40_Linear                [512, 2048]        [64, 3, 2048]          (recursive)                 1.05\n41_Dropout                         -        [64, 3, 2048]                    -                    -\n42_Linear                [2048, 512]         [64, 3, 512]          (recursive)                 1.05\n43_Dropout                         -         [64, 3, 512]                    -                    -\n44_LayerNorm                   [512]         [64, 3, 512]          (recursive)                 0.00\n45_Linear                 [512, 512]         [64, 3, 512]          (recursive)                 0.26\n46_Linear                 [512, 512]         [64, 3, 512]          (recursive)                 0.26\n47_Linear                 [512, 512]         [64, 3, 512]          (recursive)                 0.26\n48_Softmax                         -        [64, 2, 3, 3]                    -                    -\n49_Dropout                         -        [64, 2, 3, 3]                    -                    -\n50_Linear                 [512, 512]         [64, 3, 512]          (recursive)                 0.26\n51_Dropout                         -         [64, 3, 512]                    -                    -\n52_Dropout                         -         [64, 3, 512]                    -                    -\n53_LayerNorm                   [512]         [64, 3, 512]          (recursive)                 0.00\n54_Linear                [512, 2048]        [64, 3, 2048]          (recursive)                 1.05\n55_Dropout                         -        [64, 3, 2048]                    -                    -\n56_Linear                [2048, 512]         [64, 3, 512]          (recursive)                 1.05\n57_Dropout                         -         [64, 3, 512]                    -                    -\n58_LayerNorm                   [512]         [64, 3, 512]          (recursive)                 0.00\n59_Linear                 [512, 512]         [64, 3, 512]          (recursive)                 0.26\n60_Linear                 [512, 512]         [64, 3, 512]          (recursive)                 0.26\n61_Linear                 [512, 512]         [64, 3, 512]          (recursive)                 0.26\n62_Softmax                         -        [64, 2, 3, 3]                    -                    -\n63_Dropout                         -        [64, 2, 3, 3]                    -                    -\n64_Linear                 [512, 512]         [64, 3, 512]          (recursive)                 0.26\n65_Dropout                         -         [64, 3, 512]                    -                    -\n66_Dropout                         -         [64, 3, 512]                    -                    -\n67_LayerNorm                   [512]         [64, 3, 512]          (recursive)                 0.00\n68_Linear                [512, 2048]        [64, 3, 2048]          (recursive)                 1.05\n69_Dropout                         -        [64, 3, 2048]                    -                    -\n70_Linear                [2048, 512]         [64, 3, 512]          (recursive)                 1.05\n71_Dropout                         -         [64, 3, 512]                    -                    -\n72_LayerNorm                   [512]         [64, 3, 512]                 1.02                 0.00\n73_Linear               [512, 10000]       [64, 3, 10000]             5,130.00                 5.12\n====================================================================================================\n# Params:    13,403.41K\n# Mult-Adds: 25.97M\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/409917890.py:79: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler() # None\n/tmp/ipykernel_30/409917890.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  summary = torchsummaryX.summary(model.to(DEVICE), x=torch.tensor(inputs).to(DEVICE))\n","output_type":"stream"}],"execution_count":67},{"cell_type":"markdown","source":"\n\n\n\n# Trainer Class","metadata":{"id":"TlWF_bpLznup"}},{"cell_type":"code","source":"class Trainer:\n\n    def __init__(self, model, train_loader, val_loader, optimizer, criterion, scheduler, scaler, max_epochs= 1):\n\n        \"\"\"\n\n            Use this class to train your model\n\n        \"\"\"\n\n        # feel free to add any other parameters here\n\n\n        self.model      = model\n\n        self.train_loader = train_loader\n\n        self.val_loader   = val_loader\n\n        self.optimizer  = optimizer\n\n        self.criterion  = criterion\n\n        self.scheduler  = scheduler\n\n        self.scaler     = scaler\n\n\n\n        self.train_losses           = []\n\n        self.val_losses             = []\n\n        self.prediction_probs       = []\n\n        self.prediction_probs_test  = []\n\n        self.generated_texts_test   = []\n\n        self.generated_texts_test_beam = []\n\n        self.generated_texts_test_beam_random = []\n\n        self.generated_texts_validation = []\n\n\n\n        self.log_likelihood_beam = []\n\n        self.log_likelihood_beam_random = []\n\n\n\n        self.epochs                 = 0\n\n        self.max_epochs             = max_epochs\n\n\n\n\n\n    def calculate_loss(self, out, target):\n\n        # output: (B, T, Vocab_size) - probability distributions\n\n        # target: (B, T)\n\n        # Read the documentation of CrossEntropyLoss and try to understand how it takes inputs\n\n\n\n        # Tip: If your target is of shape (B, T) it means that you have B batches with T words.\n\n        # Tip: What is the total number of words in this batch?\n\n        # Tip: Crossentropy calculates the loss between a label and its probability distribution.\n\n\n\n        B, T, V = out.size()\n\n        out = out.view(-1, V)  # Flatten the output to (B*T, Vocab_size)\n\n        targets = target.view(-1)  # Flatten the target to (B*T)\n\n        loss    = self.criterion(out, targets)\n\n\n\n        return loss\n\n\n\n\n\n    def train(self):\n\n\n\n        self.model.train() # set to training mode\n\n        epoch_loss  = 0\n\n        num_batches = 0\n\n        attn_weights = None\n\n\n\n        for batch_num, (inputs, targets) in enumerate(tqdm(self.train_loader)):\n\n\n\n            # TODO: Complete the loop. You should be able to complete this without any helper comments after 3 HWs\n\n            # Tip: Use Mixed Precision Training\n\n            # For loss calculation, use the calculate_loss function. You need to complete it before using.\n\n            inputs, targets = inputs.cuda(), targets.cuda()  # Move data to GPU if available\n\n\n\n            # Mixed precision training\n\n            with torch.cuda.amp.autocast():\n\n                outputs, attn_weights = self.model(inputs)  # Forward pass\n\n                loss = self.calculate_loss(outputs, targets)  # Compute loss\n\n\n\n            self.optimizer.zero_grad()  # Zero out gradients\n\n            self.scaler.scale(loss).backward()  # Backpropagation with scaled loss\n\n            self.scaler.step(self.optimizer)  # Optimizer step\n\n            self.scaler.update()  # Update scaler for next iteration\n\n\n\n            # Update metrics\n\n            epoch_loss += loss.item()\n\n            num_batches += 1\n\n\n\n        epoch_loss = epoch_loss / (batch_num + 1)\n\n        self.epochs += 1\n\n        print('[TRAIN] \\tEpoch [%d/%d] \\tLoss: %.4f \\tLr: %.6f'\n\n                     % (self.epochs, self.max_epochs, epoch_loss, self.optimizer.param_groups[0]['lr']))\n\n        self.train_losses.append(epoch_loss)\n\n\n\n        return (epoch_loss, self.optimizer.param_groups[0]['lr'], attn_weights)\n\n\n\n    def validate(self):\n\n\n\n        self.model.eval() # set to eval mode\n\n        epoch_loss  = 0\n\n        num_batches = 0\n\n        attn_weights = None\n\n\n\n        for batch_num, (inputs, targets) in enumerate(tqdm(self.val_loader)):\n\n\n\n            # TODO: Complete the loop. You should be able to complete this without any helper comments after 3 HWs\n\n            # Tip: you don't need gradients for inference\n\n            # For loss calculation, use the calculate_loss function. You need to complete it before using it.\n\n            with torch.no_grad():  # Disable gradient computation\n\n                inputs, targets = inputs.cuda(), targets.cuda()  # Move data to GPU if available\n\n\n\n                outputs, attn_weights = self.model(inputs)  # Forward pass\n\n                loss = self.calculate_loss(outputs, targets)  # Compute loss\n\n\n\n                # Update metrics\n\n                epoch_loss += loss.item()\n\n                num_batches += 1\n\n\n\n\n\n        epoch_loss = epoch_loss / (batch_num + 1)\n\n        # self.epochs += 1\n\n        print('[VAL] \\tEpoch [%d/%d] \\tLoss: %.4f \\tLr: %.6f'\n\n                     % (self.epochs, self.max_epochs, epoch_loss, self.optimizer.param_groups[0]['lr']))\n\n        self.train_losses.append(epoch_loss)\n\n\n\n        return epoch_loss","metadata":{"executionInfo":{"elapsed":535,"status":"ok","timestamp":1732105284250,"user":{"displayName":"Pierre Ntakirutimana","userId":"14527179770260799179"},"user_tz":-120},"id":"jkFMwb095P5w","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:54:19.411812Z","iopub.execute_input":"2024-11-20T15:54:19.412147Z","iopub.status.idle":"2024-11-20T15:54:19.425604Z","shell.execute_reply.started":"2024-11-20T15:54:19.412121Z","shell.execute_reply":"2024-11-20T15:54:19.424346Z"}},"outputs":[],"execution_count":68},{"cell_type":"code","source":"# The object of the Trainer class takes in everything\n\ntrainer = Trainer(\n\n    model       = model,\n\n    train_loader = train_loader,\n\n    val_loader  = val_loader,\n\n    optimizer   = optimizer,\n\n    criterion   = criterion,\n\n    scheduler   = scheduler,\n\n    scaler      = scaler,\n\n    max_epochs  = config[\"num_epochs\"], # TODO: set the number of epochs\n\n)","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1732105285554,"user":{"displayName":"Pierre Ntakirutimana","userId":"14527179770260799179"},"user_tz":-120},"id":"2HCVG5YISwRW","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:54:19.426981Z","iopub.execute_input":"2024-11-20T15:54:19.427256Z","iopub.status.idle":"2024-11-20T15:54:19.436906Z","shell.execute_reply.started":"2024-11-20T15:54:19.427231Z","shell.execute_reply":"2024-11-20T15:54:19.436082Z"}},"outputs":[],"execution_count":69},{"cell_type":"markdown","source":"# Wandb","metadata":{"id":"Dfrf1FoSoAI0"}},{"cell_type":"code","source":"# wandb.login(key=\"d991a2801989a233cfbeaad3ed0dd2e4b2cdedfb\") # your wandb key\n# run = wandb.init(\n#     settings=wandb.Settings(symlink=False, init_timeout=200),\n#     name = \"early-submission666\", ## Wandb creates random run names if you skip this field\n#     reinit = True, ### Allows reinitalizing runs when you re-run this cell\n#     # run_id = ### Insert specific run id here if you want to resume a previous run\n#     # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n#     project = \"hw4p1-ablations\", ### Project should be created in your wandb account\n#     config = config ### Wandb Config for your run\n# )","metadata":{"executionInfo":{"elapsed":5597,"status":"ok","timestamp":1732112400815,"user":{"displayName":"Pierre Ntakirutimana","userId":"14527179770260799179"},"user_tz":-120},"id":"91s_RQaQYdy8","outputId":"607e1909-554c-4e85-a2ec-e6ec414a2647","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:54:19.438009Z","iopub.execute_input":"2024-11-20T15:54:19.438763Z","iopub.status.idle":"2024-11-20T15:54:19.446569Z","shell.execute_reply.started":"2024-11-20T15:54:19.438723Z","shell.execute_reply":"2024-11-20T15:54:19.445838Z"},"colab":{"base_uri":"https://localhost:8080/","height":156}},"outputs":[],"execution_count":70},{"cell_type":"markdown","source":"# Experiments","metadata":{"id":"0fcxKXL0hrxX"}},{"cell_type":"markdown","source":"","metadata":{"id":"n1FEuwdDcASz"}},{"cell_type":"code","source":"# Run the experiments loop.\n\n# Each epoch wont take more than 2-3min. If its taking more time, it might be due to (but not limited to) the following:\n\n#   * You might be overlapping batches\n\n#       Eg. Input: \"I had biryani for lunch today\" and sequence length = 3,\n\n#           --> \"I had biryani\", \"for lunch today\" are ideal examples for inputs\n\n#           --> \"I had biryani\", \"had biryani for\", \"biryani for lunch\", ... is just redundant info :')\n\n#   * Your length calculation in the dataloader might be wrong\n\n# If you haven't had biryani, try it :D\n\n\n\n# wandb.watch(model, log=\"all\")\n\n\n\n# torch.cuda.empty_cache()\n\ngc.collect()\n\nfor epoch in range(config['num_epochs']):\n\n    train_loss, curr_lr,  attn_weights = trainer.train()\n\n    print(attn_weights[-1].shape)\n\n\n\n    test_loss = trainer.validate()\n\n\n\n    # wandb.log({\"train_loss\":train_loss,\n\n    #            \"test_loss\": test_loss,\n\n    #            \"learning_rate\": curr_lr\n\n    #           })\n\n    scheduler.step(test_loss)\n\n\n\n### Finish your wandb run\n\n# run.finish()\n\n\n\ntorch.cuda.empty_cache()\n\ngc.collect()","metadata":{"id":"-pLh5_LCpVxw","outputId":"e0e45bcb-f127-4035-d99a-a188ca44e73f","scrolled":true,"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:54:19.447764Z","iopub.execute_input":"2024-11-20T15:54:19.448096Z"},"colab":{"base_uri":"https://localhost:8080/","height":316,"referenced_widgets":["18a3f8ce94ca464f9ee1c67c288dbc08","c5b4eeb8ebbb440f8ed1e8c27b336404","2abb070d272e4659adadf5cad1ebe1c2","eec5223042304bdab778d8e44c30bf2d","735fbbd3ce934b9eb0880f7590dcf5e0","9082a626184943c89d2ef5d7807890f9","64bdc5096d0143de95ecc4bb5f4d6592","cb2c7f6d1d5948a9951267d620c8caf8","9da5b3eaae894a138154fd8282cf0530","0168a844bdab4c31a322bc1bdd3e84ed","0317d01e93824c7a92bb8ed1acc02ee2","b243a069b4a642dc9dc2ae135c1fa192","24ed76dc05e34506bfc22251f486bbda","8ad8ef062adc402a948a80c2cdd9e012","37782ed8b4cc4319b0949ac9e07b5429","b59bee673d3e4d5ab729f7d126fd15c0","15475ac4905949f1a54c33c1e583fc8c","cb0f8efac0294562a84236a0f9590419","a262c92158884904b0d58d75112f20bd","f6cf9e70e18049c0bfb3690f0a0944c3","5214d321021d475fbeb428aa97f28468","568c9546a36941a49f2ec96f2896462e","1dbc0248a5774bd9877ae4b8be296294","4ece5ef93ab94df685d8715fd827c0e3","5a52c1b0c4d44bcaa414a0360a421db2","7659a18c5403419c84a537f99046d1b9","f496fe54db3a4cfdb351591bc8212810","af76e871cbb344d6a9cb5527b32d1361","76bc7e7341254bc394a89f428a6ec50d","b3f0a15a19fb4087a94d678261dc2e22","9d90a5ed49cf4b58870ad27ef1b56ca6","2490ba34224f454ea4f81aa5c02544e9","50da3d6c024240c3b2e05e0ed6c20c3f","00dbf3604cf040c0a3f9bd2f0bd94c48","1bc7e9baa9544c61abc0d102ab8703df","11a53878e0ef444ab48fcef8adfa7115","8120a9e572974d6d8d3fef2dfc16331d","f44f50cd6c234b1585f6ed5901aff52a","7d1d52ffb6a641bfaf16cbccfa3232c4","dc316d211f6540588495f781e42d8d24","652b648e925f4bfeb07a36ee686492bc","a6c8fba060ab4ee280225433e6aed599","68849e7132f442e2988d6c364a715ebe","23d9b835f6f74397b5f6cf2df1965718","9e669fd87fad43459d453ebc419b5e4b","6b0ad9dbb7024ccda627ce4cd8a900ee","d2c99448952249dbaee0acc06a05f9af","7a337f4ca15749c1898bd5e9752c18ae","c8da9151489541f5b532069b59402be1","b3ce44d13cee444ab7835b663b6cbf91","f19d0f3cc8b94d77bd0e0703ac14a7a6","4a8c22f39d8240aabc33b9adcd26c317","19860cc296944239a427d5c834559d56","78fe1b7fcca54930af453938e18e372b","02725876537442618e85901d2a297cc2"]}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/56540 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64742472599a46db8e35e424140b69db"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_30/4084854223.py:126: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# Testing Your Model's Perplexity","metadata":{"id":"0EScl6rumoMv"}},{"cell_type":"code","source":"def log_softmax(x, axis):\n\n    ret = x - np.max(x, axis=axis, keepdims=True)\n\n    lsm = np.log(np.sum(np.exp(ret), axis=axis, keepdims=True))\n\n    return ret - lsm\n\n\n\ndef get_prediction_nll_single_for_test(out, targ):\n\n    out = log_softmax(out, 1)\n\n    nlls = out[np.arange(out.shape[0]), targ]\n\n    nll = -np.sum(nlls)\n\n    return nll","metadata":{"id":"g7-MtnkWiHcj","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df = pd.read_csv(\"dataset/test-clean/transcripts.csv\")\n\ntest_transcripts  = []\n\ntest_transcripts = [np.array([i for i in row['transcripts'].replace(\"<sos>\", \"\").replace(\"<eos>\", \"\") ]) for index, row in test_df.iterrows()]\n\n\n\ntest_dataset = []\n\nfor files in test_transcripts:\n\n    tokenized = \"\".join(files)\n\n    tokenized = TOKENIZER.encode(tokenized)\n\n    test_dataset.append(tokenized)","metadata":{"id":"AWrNI6Ny2cyQ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dl = DataLoaderForLanguageModeling(\n\n    dataset         = test_dataset,\n\n    batch_size      = 1,\n\n    shuffle         = False,\n\n    drop_last       = False,\n\n    sequence_length = 20\n\n)\n\n\n\nnnls = []\n\nmodel.eval()\n\n\n\nfor batch_num, (inputs, targets) in enumerate(tqdm(test_dl)):\n\n    inputs = torch.tensor(inputs).long().to(DEVICE)\n\n    targets = torch.tensor(targets).long().to(DEVICE)\n\n\n\n    with torch.no_grad():\n\n        output = model(inputs)\n\n    nnl = get_prediction_nll_single_for_test(output[0][0].to('cpu').numpy(), targets[0].to('cpu').numpy())\n\n    if TOKENIZER != None:\n\n        text_len = len(TOKENIZER.decode(targets.flatten().to('cpu')).replace(\"<|endoftext|>\", \"\")) + 1\n\n    else:\n\n        text_len = len(targets[0])\n\n    nnls.append(nnl / text_len)\n\n\n\ntest_ppl = np.exp(sum(nnls) / len(nnls))\n\nprint(f'test_perplexity: {test_ppl}')\n\nwith open('./hw4/test_perplexity.txt', 'w') as f:\n\n    f.write(str(test_ppl))","metadata":{"id":"g-tmR17aiL63","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"id":"71lZ-rQ3cAS0"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"id":"Vcy0Fx5BcAS0"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}